Recent Developments in Memory Solutions for Agentic AI, RAG, and LLMs
Introduction: The Paramountcy of Memory in Advanced AI Systems
Memory stands as a foundational element for the progression of sophisticated artificial intelligence systems, particularly those based on large language models (LLMs) and employed in agentic frameworks or retrieval-augmented generation (RAG) pipelines.1 The capacity of these systems to sustain coherent dialogues, engage in prolonged interactions, and execute intricate tasks hinges critically on their ability to effectively store, retrieve, and utilize information over time.1 Various mechanisms, including memory storage, retrieval, and memory-grounded generation, are underpinned by this crucial component, enabling LLM-based AI to move beyond mere pattern recognition towards more nuanced and context-aware processing.1 The ongoing research in this domain reflects a deep understanding that advancements in memory are not merely incremental improvements but are essential for unlocking the next level of intelligence in artificial systems.
The concept of eidetic memory, often idealized as a perfect photographic recall, serves as a compelling benchmark, representing the ultimate aspiration for memory capabilities in artificial intelligence.3 While the precise nature and prevalence of true eidetic memory in humans remain subjects of debate, the underlying principle of highly detailed and persistent recall continues to inspire researchers in their pursuit of enhanced memory systems for AI.3 Recent developments, such as the introduction of features like Microsoft's "Recall," which aims to provide users with a comprehensive and searchable history of their computer activity, underscore a growing interest in achieving more persistent and detailed forms of memory in AI applications, even if they fall short of true eidetic capabilities.3 This drive towards improved recall highlights the practical value and user demand for AI systems that can effectively remember and access past information with high fidelity.
This report endeavors to provide a comprehensive overview of the recent (2024-2025) progress in memory solutions tailored for Agentic AI, RAG, and LLMs. It will delve into the current limitations of memory in these systems, explore innovative memory architectures and modules that have emerged, analyze approaches inspired by the concept of eidetic memory, and discuss memory solutions specifically designed for agentic systems and retrieval-augmented generation. Furthermore, the report will examine advancements in memory capabilities for large language models and identify converging themes and future directions in this rapidly evolving field.
Deciphering the Labyrinth: Current Memory Limitations in Agentic AI, RAG, and LLMs
A fundamental constraint in the architecture of many large language models is the presence of a fixed-length context window.8 This limitation dictates the maximum number of tokens an LLM can process in a single input, thereby restricting the amount of information it can effectively retain and utilize within a given interaction.8 Consequently, the ability of these models to process or recall long sequences of information over extended periods is inherently hindered.8 This architectural bottleneck necessitates the development of external memory solutions or sophisticated techniques to augment the LLM's capacity to remember information beyond its immediate processing window.
Maintaining long-term coherence and achieving persistent recall across extended interactions present significant challenges for current LLMs.8 Often, these models operate in a stateless manner, meaning they do not inherently remember past conversations or interactions unless the entire history is explicitly provided within the current prompt.8 Unlike the human brain, which possesses mechanisms for consolidating episodic memories into long-term storage, LLMs typically lack such internal processes.8 This absence of a built-in long-term memory capability makes it difficult for LLMs to engage in truly sustained and context-aware interactions over multiple sessions.
Another critical area of concern involves knowledge integration, the dynamic updating of information, and the potential for catastrophic forgetting.8 Continual learning, where AI models learn new information without losing previously acquired knowledge, remains a substantial hurdle for LLMs.8 Selectively updating the vast number of parameters within these models to incorporate new data or experiences can be computationally expensive and often leads to the model inadvertently forgetting previously learned information, a phenomenon known as catastrophic forgetting.1 Retrieval-augmented generation has emerged as a prominent strategy to address this challenge by allowing LLMs to access external knowledge sources at inference time, thus providing them with up-to-date information without requiring extensive retraining of the model's internal parameters.
Agentic AI systems encounter specific memory-related challenges that extend beyond conversational context, particularly in the domains of action planning and the meticulous tracking of environmental states.10 For an agent to autonomously navigate and operate within an environment, it needs to remember its overarching goals, the sequence of actions it has undertaken, and the current state of the world around it.10 Both short-term memory, for managing the immediate context of a task, and long-term memory, for recalling past experiences and accumulated knowledge, are crucial for enabling effective agentic behavior.10 The memory requirements for agentic AI thus involve not only storing textual information but also maintaining structured data about actions, observations, and environmental changes to support sequential reasoning and goal-oriented behavior.
In the realm of Retrieval-Augmented Generation, several memory-centric bottlenecks can impede performance, with retrieval relevance and the seamless integration of retrieved knowledge into the generation process being particularly critical.9 The efficacy of RAG systems is heavily reliant on their ability to retrieve the most pertinent information from a potentially vast knowledge base in response to a user's query.9 Furthermore, even when relevant information is retrieved, challenges remain in effectively incorporating this external knowledge into the LLM's generation process in a coherent and contextually appropriate manner.9 Standard RAG approaches may face difficulties when dealing with ambiguous user queries or tasks that necessitate multi-hop reasoning, where the answer requires synthesizing information from multiple retrieved documents.9 Therefore, advancements in both the precision of information retrieval and the mechanisms for integrating retrieved content are essential for improving the overall performance and reliability of RAG systems.
Architecting Recall: Recent Advancements in Memory Architectures and Modules (2024-2025)
Recent years have witnessed a surge in research focused on developing innovative memory modules and architectural designs aimed at enhancing both the capacity and the operational efficiency of memory systems for advanced AI. These efforts span various approaches, from refining the fundamental operations of memory management to introducing entirely new paradigms for how AI systems store and access information.
One significant area of advancement is the development of agentic memory systems, which empower dynamic structuring and the organic evolution of memory within LLM agents.13 A notable example is A-MEM, an agentic memory system inspired by the Zettelkasten method, a sophisticated knowledge management system that creates interconnected information networks.13 A-MEM enables the autonomous generation of contextual descriptions for memories, facilitates the dynamic establishment of connections between memories based on shared attributes and contextual similarities, and supports the intelligent evolution of existing memories in response to new experiences.13 This agentic approach allows LLM agents to actively participate in organizing and linking their memories, moving beyond static, predefined memory structures towards more flexible and adaptive knowledge networks.14
A structured understanding of memory in LLM-based agents can be achieved by examining the fundamental memory management operations that underpin these systems: consolidation, updating, indexing, strategic forgetting, efficient retrieval, and intelligent compression.1 Consolidation involves transforming short-term experiences into persistent memory, encoding interaction histories into durable forms.1 Updating modifies existing memory representations in response to new data, refining or adjusting stored information.1 Indexing efficiently organizes memory for retrieval by creating auxiliary codes that serve as access points to stored content.1 Forgetting strategically suppresses outdated or irrelevant memory content, ensuring that the system focuses on the most pertinent information.1 Retrieval accesses relevant memory content when needed, based on current inputs or internal states.1 Finally, compression reduces memory size while preserving essential information, enabling efficient storage and reasoning.1 Analyzing memory through these atomic operations provides a granular perspective for understanding and improving memory management in AI systems.
Furthermore, memory in LLM-based agents can be categorized into distinct types, each with its own characteristics and temporal scope: parametric memory, contextual unstructured memory, and contextual structured memory.1 Parametric memory refers to the knowledge implicitly stored within the model's weights, acquired during pretraining or post-training, enabling fast retrieval of factual information.1 Contextual unstructured memory is an explicit system that stores and retrieves information across various modalities like text, images, and video, allowing agents to ground reasoning in perceptual signals.1 Contextual structured memory involves explicit memory organized into predefined formats such as knowledge graphs and relational tables, supporting symbolic reasoning and precise querying.1 These memory types can be further classified by their temporal span, distinguishing between short-term memory, which typically corresponds to transient, session-level contexts, and long-term memory, which encompasses persistent information stored across sessions.1 Recognizing the strengths and weaknesses of each memory type is crucial for designing effective and versatile AI systems.
Table 1: Comparison of Memory Representations in LLM-based Agents

Memory Representation Type	Description	Temporal Span	Key Characteristics/Strengths	Limitations	Example Snippets
Parametric Memory	Knowledge embedded in model weights	Long-Term, Persistent	Fast, context-free retrieval of factual and common-sense knowledge	Lacks transparency, difficult to update selectively	1
Contextual Unstructured Memory	Explicit system storing and retrieving information across modalities (text, image, audio, video)	Short-Term (session-level), Long-Term (cross-session)	Enables grounding reasoning in perceptual signals, integrates multi-modal context	Can be less structured, retrieval might rely on similarity matching	1
Contextual Structured Memory	Explicit memory organized into predefined formats (knowledge graphs, relational tables, ontologies)	Short-Term (inference-time), Long-Term (curated across sessions)	Supports symbolic reasoning, precise querying, complements associative capabilities of PLMs	Requires predefined schemas, might not capture all types of information	1
Table 2: Key Memory Operations in LLM-based Agents

Memory Operation	Description	Purpose/Benefit	Example Snippets
Consolidation	Transforms short-term experiences into persistent memory	Enables continual learning and personalization	1
Updating	Modifies existing memory representations in response to new data	Allows for adaptation and refinement of stored information	1
Indexing	Efficiently organizes memory for retrieval using auxiliary codes	Enables fast and semantically coherent access to stored content	1
Forgetting	Selectively suppresses outdated or irrelevant memory content	Improves efficiency and focuses the system on pertinent information	1
Retrieval	Accesses relevant memory content when needed	Provides necessary information for reasoning and generation	1
Compression	Reduces memory size while preserving essential information	Enables efficient storage and processing	1
Echoes of Experience: Eidetic Memory-Inspired Approaches and Persistent Recall Solutions
Recent research has explored the potential for achieving highly detailed and persistent recall capabilities in artificial intelligence systems, drawing inspiration, in part, from the human phenomenon of eidetic memory. While replicating true eidetic memory remains a significant challenge, these endeavors represent a step towards enhancing the recall abilities of AI.
One notable development in the pursuit of enhanced AI memory is Microsoft's "Recall" feature, introduced for Copilot+ PCs.3 This feature takes continuous snapshots of virtually everything a user sees and does on their Windows PC, aiming to provide a searchable history of their computer activity, akin to a "photographic memory".3 Users can then search their past activities using natural language to quickly find specific documents, websites, or email threads.4 While "Recall" operates at the operating system level rather than directly within LLMs or agentic systems, its development reflects a growing user expectation for AI to remember past interactions and information in a highly detailed and accessible manner.3 The introduction of such a feature, despite raising significant privacy and security concerns, highlights a practical interest in achieving persistent recall in digital environments, which could indirectly influence the design and expectations for memory solutions in AI agents and LLMs.
Achieving genuine eidetic memory capabilities within Agentic AI, RAG, and LLMs presents formidable challenges.5 Eidetic memory in humans involves the ability to recall mental images, sounds, or objects in great detail after only a short exposure.5 True eidetic memory is considered a rare phenomenon and may decline with age.5 It is also important to distinguish between eidetic memory, where one might "see" an image-like memory, and photographic memory, which involves the recall of specific visual details without necessarily "seeing" the image.5 Given the current understanding of human cognition, replicating such a complex and possibly innate ability in artificial systems is likely a long-term endeavor. Current research in AI memory is more realistically focused on achieving incremental improvements in memory capacity, persistence, and retrieval efficiency, rather than a perfect imitation of human eidetic memory. However, the aspiration for such detailed and persistent recall continues to drive innovation in the field.
Memory as the Bedrock of Agency: Solutions for Agentic AI
Memory is an indispensable component for Agentic AI systems, underpinning their ability to reason, plan, and act autonomously.18 These agentic LLMs are characterized by their capacity to perceive their environment, make decisions, and execute actions to achieve specific goals.18 For such agents to function effectively, they must possess a robust memory system that allows them to maintain context across interactions and learn from past experiences.10 Self-reflective agents, for instance, leverage memory to evaluate their previous actions, identify areas for improvement, and refine their strategies for future tasks.18
Recent advancements emphasize the crucial role of memory in enabling agents to execute tasks effectively, interact seamlessly with external environments, and continuously learn from accumulated experiences.20 The concept of experiential learning, as highlighted by ExpeL (Experiential Learner), underscores the importance of agents consolidating their past interactions and using these experiences to guide future behavior.20 Research into Agent Workflow Memory explores methods for meticulously tracking the steps an agent takes and the context surrounding those actions, providing a detailed record that can be used for analysis and improvement.22 Memory also serves as a repository for retaining context and relevant data across multiple interactions, allowing agents to build upon past knowledge and maintain coherence over time.11
Furthermore, memory plays a critical role in the context of multi-agent systems and collaborative task completion.18 LLM-based Multi-Agent Systems (MASs) harness the collective intelligence of multiple LLMs to tackle complex challenges that may be beyond the scope of a single agent.23 In such systems, memory enables distributed agents to retain and share diverse knowledge bases, preventing the overload of any single component.23 Effective collaboration within MASs necessitates the development of mechanisms that allow agents to share their memories, coordinate their actions, and remember the progress of the joint effort towards achieving common goals.23 This includes managing shared knowledge repositories and establishing communication protocols that leverage memory to ensure a cohesive and efficient collaborative process.
Augmenting Recall: Memory Solutions for Retrieval-Augmented Generation (RAG)
Memory mechanisms are increasingly being integrated into Retrieval-Augmented Generation (RAG) methodologies to enhance long-context processing and improve overall performance.12 One notable advancement in this area is MemoRAG, a novel RAG framework that features a dual-system architecture.25 This framework employs a light but long-range system to create a global memory of the long context, which then generates draft answers that serve as clues for the retrieval tools to locate relevant information.25 MemoRAG aims to address the limitations of traditional RAG in handling extended contexts and ambiguous information needs by providing a more comprehensive understanding of the underlying data before the retrieval process.12
Beyond dual-system architectures, memory-augmented retrieval techniques are being developed to achieve improved accuracy and enhanced relevance in information retrieval within RAG frameworks.9 Structure-augmented RAG methods, for example, go beyond simple vector embeddings by incorporating structures like knowledge graphs to enhance sense-making and associativity.9 Systems like HippoRAG and HippoRAG 2 demonstrate the potential of such approaches to improve performance across various memory-related tasks, bringing RAG closer to the dynamic and interconnected nature of human long-term memory.9 MemoRAG itself can be considered a form of memory-augmented retrieval, where the initial "memory" of the context guides the subsequent retrieval of more specific information.25
Memory is also being effectively utilized within RAG frameworks to handle ambiguous user queries and process unstructured knowledge sources.12 MemoRAG, in particular, is designed to address tasks with ambiguous information needs by leveraging its global memory to generate draft answers that provide clues for the retrieval tools to identify relevant content.12 The memory module in MemoRAG is specifically intended to be instructive, generating useful clues based on a compressed representation of the knowledge source, thereby enabling more comprehensive retrieval even when the user's query is not perfectly clear or when the required information is spread across multiple documents.25
Table 3: Comparison of Traditional RAG and Agentic RAG
Feature	Traditional RAG	Agentic RAG
Autonomy	Low; primarily reactive based on user input	Medium to High; incorporates autonomous decision-making and planning
Context Awareness	Medium; through embeddings of retrieved documents	High; with memory retention functionalities (both short-term and long-term)
Memory	Limited to the context window of the LLM and retrieved documents	Dynamic and evolves in real-time using both long and short-term memory; can store past interactions and user preferences
Interaction Style	Query-based information retrieval	Dialogue style interaction with multiple questions and answers; can proactively seek information
Human Intervention	Moderate; may need human refinement of generated input	Very Low; self-refines output and can automate knowledge base management
Workflow	Static, one-step retrieval process	Dynamic, multi-step workflows with iterative refinement and potential use of tools
Remembering the Past, Shaping the Future: Memory Solutions for Large Language Models (LLMs)
Significant research efforts are directed towards overcoming the context window limitations of large language models and substantially improving their capacity for long-term memory retention.25 Recent advancements have enabled some LLMs to handle considerably longer contexts, with window sizes expanding to 32K or even 128K tokens.25 MemoRAG can be viewed as one strategic approach to address the challenges of processing long contexts in LLMs by employing a memory-augmented retrieval mechanism.25 Furthermore, InfLLM presents a novel training-free, memory-based method for extending the effective context window of LLMs, allowing them to process longer sequences without requiring additional fine-tuning.34
Achieving efficient memory management and optimal utilization of memory resources within the architectural constraints of LLMs is another critical area of focus.2 Research on efficient LLMs explores various techniques for handling long contexts, including parameter-efficient fine-tuning and memory-retrieval augmentation.36 Specific methods such as KV cache dropping, storing optimization, and selection are being investigated to enhance parametric efficiency when dealing with long-context memory.2 Additionally, context retrieval and compression techniques are being explored to improve the effectiveness of contextual utilization in scenarios involving extended sequences.2
Emerging approaches are also focusing on seamlessly integrating external knowledge and diverse experiences into the memory framework of LLMs.9 Projects like AI-native Memory 2.0, also known as Second Me, aim to create personalized AI applications by enhancing structured knowledge organization and contextual reasoning within LLMs, effectively acting as an extension of human memory.38 Similarly, HippoRAG 2 has demonstrated improved performance in tasks that require the integration of information from disparate passages, showcasing advancements in how LLMs can leverage external knowledge across various memory-intensive tasks.9
Convergence and Innovation: Cross-Cutting Themes and Emerging Trends
Several overarching trends and shared innovative approaches are evident across the domains of Agentic AI, RAG, and LLM memory solutions. A prominent theme is the increasing focus on long-context processing as a fundamental challenge that needs to be addressed to unlock the full potential of these advanced AI systems. The growing adoption of agentic principles for memory management, as seen in systems like A-MEM, signifies a move towards more autonomous and adaptive AI that can dynamically organize and evolve its knowledge over time. There is also a clear recognition of the need for memory systems to handle diverse modalities beyond just text, enabling AI to process and remember information from images, audio, and video. Furthermore, the development of dual-system architectures, such as in MemoRAG, which separate the processes of memory creation or retrieval from the core LLM processing, represents a significant architectural innovation aimed at improving efficiency and performance.
Insights from cognitive psychology continue to play a significant role in inspiring and guiding advancements in AI memory research.1 The categorization of memory into short-term and long-term, as well as the exploration of different types of memory such as episodic, semantic, and procedural, often draw inspiration from human cognitive models.1 Concepts like memory consolidation and forgetting, which are extensively studied in psychology, are also being explored in the context of AI memory to improve learning and efficiency.1 This interdisciplinary approach highlights the value of understanding human memory mechanisms in the quest to build more sophisticated and effective memory systems for artificial intelligence.
Finally, the development of robust evaluation benchmarks specifically designed to assess the diverse capabilities of memory systems in these advanced AI applications is of crucial importance.32 Benchmarks like LongBench and InfiniteBench have emerged to evaluate the ability of LLMs to handle long contexts, while CORAL benchmarks the performance of multi-turn conversational retrieval-augmentation generation.12 The growing recognition of the need for unified evaluation processes for RAG systems further underscores the importance of having standardized metrics to measure progress in this field.39 These benchmarks are essential for objectively comparing different memory solutions and for guiding future research and development efforts towards more capable and reliable AI systems.
Conclusion: Charting the Course for Future Recall in AI
The recent advancements in memory solutions for Agentic AI, RAG, and LLMs signify a dynamic and rapidly evolving field. The paramount importance of memory for coherent interaction, complex task execution, and learning has driven significant innovation across various architectural levels and operational paradigms. From agentic memory systems that dynamically structure knowledge to memory-augmented RAG frameworks that enhance retrieval precision and LLMs with extended context windows, the progress made in 2024-2025 demonstrates a clear trajectory towards more sophisticated and human-like memory capabilities in artificial intelligence.
The pursuit of eidetic-like recall, while still facing considerable challenges, continues to inspire research and development, as evidenced by projects aiming for highly detailed and persistent digital memories. Memory solutions tailored for agentic systems are focusing on enabling autonomous reasoning, planning, and learning from experience, while advancements in RAG are enhancing the ability of LLMs to leverage external knowledge effectively, even for ambiguous queries and unstructured data. Furthermore, efforts to overcome the inherent context window limitations of LLMs and to efficiently manage memory resources are paving the way for more practical applications of these powerful models in real-world scenarios.
Looking ahead, future research is likely to continue exploring the integration of insights from cognitive psychology to guide the design of AI memory systems. The development of more comprehensive and standardized evaluation benchmarks will be crucial for measuring progress and ensuring the reliability of new memory solutions. As AI systems become increasingly integrated into various aspects of our lives, the ability to remember, learn, and reason effectively will be paramount, making the ongoing advancements in memory solutions a critical factor in shaping the future landscape of intelligent artificial intelligence.
Works cited
1.	arxiv.org, accessed on May 7, 2025, https://arxiv.org/pdf/2505.00675
2.	Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2505.00675v1
3.	Microsoft adds three new AI features to Copilot+ PCs - including the controversial Recall, accessed on May 7, 2025, https://www.zdnet.com/article/microsoft-adds-three-new-ai-features-to-copilot-pcs-including-the-controversial-recall/
4.	Microsoft will revive 'privacy nightmare' AI screenshot tool Recall on certain Windows 11 devices within days - GB News, accessed on May 7, 2025, https://www.gbnews.com/tech/microsoft-recall-copilot-plus-pc-release-date
5.	Eidetic Memory: Can You Train Your Brain To Remember Better? - BetterUp, accessed on May 7, 2025, https://www.betterup.com/blog/eidetic-memory
6.	Kruel.ai V7.0 - Api companion with full understanding with persistent memory - Community, accessed on May 7, 2025, https://community.openai.com/t/kruel-ai-v7-0-api-companion-with-full-understanding-with-persistent-memory/674592
7.	Microsoft's new Recall AI takes constant screenshots | Information Age - ACS, accessed on May 7, 2025, https://ia.acs.org.au/article/2024/microsoft-s-new-ai-recall-takes-constant-screenshots.html
8.	Procedural Memory Is Not All You Need - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2505.03434v1
9.	From RAG to Memory: Non-Parametric Continual Learning for Large Language Models, accessed on May 7, 2025, https://arxiv.org/html/2502.14802v1
10.	LLM Agents - Prompt Engineering Guide, accessed on May 7, 2025, https://www.promptingguide.ai/research/llm-agents
11.	Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2501.09136v1
12.	Hongjin Qian 钱泓锦 | Homepage, accessed on May 7, 2025, https://qhjqhj00.github.io/
13.	arxiv.org, accessed on May 7, 2025, http://arxiv.org/pdf/2502.12110
14.	A-Mem: Agentic Memory for LLM Agents - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2502.12110v1
15.	A-MEM: Agentic Memory for LLM Agents - alphaXiv, accessed on May 7, 2025, https://www.alphaxiv.org/overview/2502.12110
16.	A-MEM: Agentic Memory for LLM Agents // April Reading Group - Video | MLOps Community, accessed on May 7, 2025, https://home.mlops.community/public/videos/a-mem-agentic-memory-for-llm-agents-april-reading-group
17.	[2502.12110] A-MEM: Agentic Memory for LLM Agents - arXiv, accessed on May 7, 2025, https://arxiv.org/abs/2502.12110
18.	Agentic Large Language Models, a survey - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2503.23037v2
19.	A Survey on Large Language Model based Human-Agent Systems - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2505.00753v1
20.	About Awesome things towards foundation agents. Papers / Repos / Blogs - GitHub, accessed on May 7, 2025, https://github.com/FoundationAgents/awesome-foundation-agents
21.	Manling Li, accessed on May 7, 2025, https://limanling.github.io/
22.	Daniel Fried, accessed on May 7, 2025, https://dpfried.github.io/
23.	Multi-Agent Collaboration Mechanisms: A Survey of LLMs - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2501.06322v1
24.	Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective - ResearchGate, accessed on May 7, 2025, https://www.researchgate.net/publication/390545170_Inherent_and_emergent_liability_issues_in_LLM-based_agentic_systems_a_principal-agent_perspective
25.	MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2409.05591v3
26.	MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery - Paper Details - ChatPaper.ai, accessed on May 7, 2025, https://www.chatpaper.ai/dashboard/paper/5088b619-c2a0-4e51-8815-c02c8208c4ad
27.	MemoRAG: A Memory-Enhanced Approach to Next-Gen RAG - Stephen Collins.tech, accessed on May 7, 2025, https://stephencollins.tech/newsletters/memorag-memory-enhanced-approach-to-rag
28.	MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery, accessed on May 7, 2025, https://powerdrill.ai/discover/discover-MemoRAG-Moving-towards-cm0wx1y3yrbbm019wkfg15gqn
29.	[2409.05591] MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation - arXiv, accessed on May 7, 2025, https://arxiv.org/abs/2409.05591
30.	memorag: moving towards next-gen rag via memory-inspired knowledge discovery - Nettpilot, accessed on May 7, 2025, https://nettpilot.no/wp-content/uploads/2024/10/memorag-2409.05591v2.pdf
31.	Shifting Long-Context LLMs Research from Input to Output - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2503.04723
32.	BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack, accessed on May 7, 2025, https://neurips.cc/virtual/2024/poster/97462
33.	Thus Spake Long-Context Large Language Model - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2502.17129v1
34.	NeurIPS Poster InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory, accessed on May 7, 2025, https://neurips.cc/virtual/2024/poster/94480
35.	InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory | OpenReview, accessed on May 7, 2025, https://openreview.net/forum?id=bTHFrqhASY
36.	AIoT-MLSys-Lab/Efficient-LLMs-Survey: [TMLR 2024] Efficient Large Language Models, accessed on May 7, 2025, https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey
37.	Xnhyacinth/Awesome-LLM-Long-Context-Modeling: Must-read papers and blogs on LLM based Long Context Modeling - GitHub, accessed on May 7, 2025, https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling
38.	arXiv:2503.08102v2 [cs.AI] 12 Mar 2025, accessed on May 7, 2025, https://arxiv.org/pdf/2503.08102
39.	Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook, accessed on May 7, 2025, https://arxiv.org/html/2503.18016v1
40.	iwangjian/Paper-Reading-ConvAI: Paper reading list in conversational AI. - GitHub, accessed on May 7, 2025, https://github.com/iwangjian/Paper-Reading-ConvAI
41.	NAACL2025 Tutorial: Adaptation of Large Language Models - arXiv, accessed on May 7, 2025, https://arxiv.org/html/2504.03931v2

