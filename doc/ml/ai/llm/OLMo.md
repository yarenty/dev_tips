# OLMo - Open Language Models


We introduce OLMo 2 1B, the smallest model in the OLMo 2 family. OLMo 2 was pre-trained on OLMo-mix-1124 and uses Dolmino-mix-1124 for mid-training.

OLMo 2 is the latest in a series of Open Language Models designed to enable the science of language models. We have released all code, checkpoints, logs, and associated training details on GitHub.


https://huggingface.co/allenai/OLMo-2-0425-1B?utm_source=tldrai


## Github

https://github.com/allenai/OLMo?tab=readme-ov-file


## DATA
https://huggingface.co/datasets/allenai/dolmino-mix-1124