# Memory


Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future  Directions
https://arxiv.org/pdf/2505.00675



Use the tools to answer the question: Has anyone looking into Agentic AI/RAG and memory solution, could be eidetic memory great solution for Agents/RAG/LLM. could you try find examples like in arXiv or PubMed , could you focus on memory issues / solutions - especially within latest years (2024/20205)


# Task
Has anyone looking into Agentic AI/RAG and memory solution,  could be eidetic memory great solution for Agents/RAG/LLM.  could you try find examples like in arXiv or PubMed , could you focus on memory issues / solutions - especially within latest years (2024/20205)

# Response
Yes. Multiple recent studies have explored innovative memory solutions for Agentic AI, RAG systems, and LLMs that incorporate dynamic architectures and retrieval methods designed to emulate aspects of human eidetic memory (cheng2024exploringlargelanguage pages 12-13). One set of works introduces memory systems that separate transient short-term memory from a long-term knowledge repository, where essential information is consolidated and dynamically updated in a manner reminiscent of eidetic recall (chhikara2025mem0buildingproductionready pages 1-3, chhikara2025mem0buildingproductionready pages 14-16). Other research explicitly highlights the goal of approaching human‐like memory precision by combining structured representations and continuous memory refinement—a core facet of eidetic memory—in agents that utilize RAG frameworks and adaptive memory retrieval (chhikara2025mem0buildingproductionready pages 20-23, chhikara2025mem0buildingproductionready pages 23-23).

Research focusing on Agentic AI has detailed methods that integrate both parametric and external memory representations while employing operations such as addition, updating, and selective retrieval. Such structured approaches create hierarchies that support the retention of detailed episodic experiences and factual observations, which are features attributed to eidetic memory (du2025rethinkingmemoryin pages 13-14, du2025rethinkingmemoryin pages 14-15). Complementary studies describe techniques that extend the short context capabilities of LLMs through efficient memory compression and retrieval strategies, thus ensuring that essential information remains accessible and accurate over long sequences—one of the desired characteristics of an eidetic system (du2025rethinkingmemoryin pages 6-7, feng2024howfararea pages 12-14).

Additional investigations have probed memory efficiency and the balance between computational cost and memory retention. For instance, some models achieve significant reductions in latency and token usage by leveraging graph-based and hierarchical memory representations, which further support the iterative updating and contextual refinement required to simulate eidetic memory (liu2025advancesandchallenges pages 201-202, liu2025advancesandchallenges pages 202-204). Other contributions from the literature emphasize the integration of layered memory approaches—encompassing sensory, short-term, and long-term components—to consolidate and retrieve information reliably across extended interactions (liu2025advancesandchallenges pages 204-206, liu2025advancesandchallenges pages 43-45).

Moreover, broader surveys on large language models and AI memory underscore that memory systems are increasingly being designed to not only store and manage data but also to refine and update internal knowledge representations continually. This trend is driven by the need to overcome the inherent limitations of fixed context windows in traditional LLMs and to endow agents with properties analogous to eidetic memory (liu2025advancesandchallenges pages 52-53, mumuni2501largelanguagemodels pages 19-20).

In summary, the collected body of work from 2024 to 2025 demonstrates that researchers have indeed targeted the integration of advanced memory solutions—featuring dynamic, scalable, and contextually rich architectures—as a means to approximate eidetic memory capabilities in Agentic AI, RAG systems, and LLMs (cheng2024exploringlargelanguage pages 12-13, liu2025advancesandchallenges pages 52-53).

## References

1. (cheng2024exploringlargelanguage pages 12-13): Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. Exploring large language model based intelligent agents: definitions, methods, and prospects. ArXiv, Jan 2024. URL: https://doi.org/10.48550/arxiv.2401.03428, doi:10.48550/arxiv.2401.03428. This article has 89 citations.

2. (chhikara2025mem0buildingproductionready pages 1-3): P Chhikara. Mem0: building production-ready ai agents with scalable long-term memory. ArXiv, 2025. URL: https://doi.org/10.48550/arxiv.2504.19413, doi:10.48550/arxiv.2504.19413.


6. (du2025rethinkingmemoryin pages 13-14): Y Du, W Huang, D Zheng, and S Montella. Rethinking memory in ai: taxonomy, operations, topics, and future directions. ArXiv, 2025. URL: https://doi.org/10.48550/arxiv.2505.00675, doi:10.48550/arxiv.2505.00675.

9. (feng2024howfararea pages 12-14): T Feng, C Jin, J Liu, K Zhu, H Tu, and Z Cheng. How far are we from agi: are llms all we need? Unknown journal, 2024.

10. (liu2025advancesandchallenges pages 201-202): B Liu, J Zhang, and J Wang. Advances and challenges in foundation agents: from brain-inspired intelligence to evolutionary, collaborative, and safe systems. ArXiv, 2025. URL: https://doi.org/10.48550/arxiv.2504.01990, doi:10.48550/arxiv.2504.01990.


15. (mumuni2501largelanguagemodels pages 19-20): F Mumuni A Mumuni. Large language models for artificial general intelligence (agi): a survey of foundational principles and approaches. ArXiv, Jan 2501. URL: https://doi.org/10.48550/arxiv.2501.03151, doi:10.48550/arxiv.2501.03151. This article has 2 citations.


[awesome adent fundations](https://github.com/FoundationAgents/awesome-foundation-agents?tab=readme-ov-file#memory)



PAPERS:

[](query: Agentic AI memory solutions OR RAG memory solutions OR eidetic memory for agents OR LLM memory architectures OR persistent memory in RAG OR memory augmentation in LLM OR memory in autonomous AI agents OR scalable memory for Retrieval-Augmented Generation, min_year: 2024, max_year: 2025

1

Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory

P ChhikaraArXiv, 2025
2

Large Model Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends

Y Wang, Z Su, L DuArXiv, 2024citations 9
3

Lifelong Learning of Large Language Model based Agents: A Roadmap

Junhao Zheng, Chengming Shi, Xidi Cai, Qiuke Li, Duzhen Zhang, Chenxing Li, Dong Yu, Qianli MaArXiv, Jan 2025citations 5
4

How Far Are We From AGI

Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, Jiaxuan YouArXiv, May 2024citations 23
5

Zep: A Temporal Knowledge Graph Architecture for Agent Memory

Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, Daniel ChalefArXiv, Jan 2025citations 2
6

Practical Considerations for Agentic LLM Systems

Chris Sypherd, Vaishak BelleArXiv, Dec 2024citations 2
7

Large Language Model Agent: A Survey on Methodology, Applications and Challenges

Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiaoming Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Mengxue Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xianhong Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, Ming ZhangArXiv, Mar 2025citations 3
8

From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs

Y Wu, C Zhang, Y WangArXiv, 2025
9

From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review

MA Ferrag, N Tihanyi, M DebbahArXiv, 2504
10

Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG

Aditi Singh, Abul Ehtesham, Saket Kumar, T. T. KhoeiArXiv, Jan 2025citations 14
11

Agentic Systems: A Guide to Transforming Industries with Vertical AI Agents

Fouad BousetouaneArXiv, Jan 2025citations 6
12

A Survey on Knowledge-Oriented Retrieval-Augmented Generation

Mingyue Cheng, Yucong Luo, Ouyang Jie, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, Enhong ChenArXiv, Mar 2025citations 1
13

Generative to Agentic AI: Survey, Conceptualization, and Challenges

J SchneiderArXiv, 2504
14

A Survey of Personalization: From RAG to Agent

X Li, P JiaArXiv, 2025
15

Agentic Large Language Models, a survey

A. Plaat, M. V. Duijn, N. V. Stein, Mike Preuss, P. V. D. Putten, K. BatenburgArXiv, Mar 2025citations 2
16

Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models

Chengkai Huang, Junda Wu, Yu Xia, Zixu Yu, Ruhan Wang, Tong Yu, Ruiyi Zhang, Ryan A. Rossi, B. Kveton, Dongruo Zhou, Julian J. McAuley, Lina YaoArXiv, Mar 2025citations 1
17

AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents

Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, M. Burtsev, Evgeny BurnaevArXiv, July 2024citations 13
18

Agentic Information Retrieval

Weinan Zhang, Junwei Liao, Ning Li, Kounianhua DuArXiv, Oct 2024citations 8
19

A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms

C Huang, K Xie, J Wu, S ZhangArXiv, 2025
20

Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents

Mathis Pink, Qinyuan Wu, Vy A. Vo, Javier S. Turek, Jianing Mu, Alexander Huth, Mariya TonevaArXiv, Feb 2025citations 2
21

Towards a Science Exocortex

Kevin G. YagerArXiv, June 2024citations 4
22

A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models

Qinggang Zhang, Shengyuan Chen, Yuan-Qi Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Junnan Dong, Hao Chen, Yi Chang, Xiao HuangArXiv, Jan 2025citations 14
23

Generative AI Agents in Autonomous Machines: A Safety Perspective

Jason Jabbour, Vijay Janapa ReddiProceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design, Oct 2024citations 4
24

Graph-based Approaches and Functionalities in Retrieval-Augmented Generation: A Comprehensive Survey

T Huang, K WangArXiv, 2025
25

Graph Retrieval-Augmented Generation: A Survey

Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang TangArXiv, Aug 2024citations 88
26

A Survey of Scaling in Large Language Model Reasoning

Z Chen, S Wang, Z Tan, X Fu, Z Lei, P WangArXiv, 2025
27

AI Agents: Evolution, Architecture, and Real-World Applications

Naveen KrishnanArXiv, Mar 2025citations 1
28

Generative AI and LLMs for Critical Infrastructure Protection: Evaluation Benchmarks, Agentic AI, Challenges, and Opportunities

Yagmur Yigit, Mohamed Amine Ferrag, Mohamed C. Ghanem, Iqbal H. Sarker, Leandros A. Maglaras, Christos Chrysoulas, Naghmeh Moradpoor, Norbert Tihanyi, Helge JanickeSensors, Mar 2025
PEER REVIEWED

citations 2
29

Synergizing RAG and Reasoning: A Systematic Review

Y Gao, Y Zhong, Y BiArXiv, 2025
30

A Survey on the Optimization of Large Language Model-based Agents

Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, Liang HeArXiv, Mar 2025
31

Reasoning Beyond Limits: Advances and Open Problems for LLMs

M. Ferrag, Norbert Tihanyi, M. DebbahArXiv, Mar 2025citations 3
32

Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation

Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, M. Baghshah, Ehsaneddin AsgariArXiv, Feb 2025citations 3
33

Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook

Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Lutao Jiang, Haiwei Xue, Bin Ren, D. Paudel, N. Sebe, L. V. Gool, Xuming HuArXiv, Mar 2025citations 1
34

Physical AI Agents: Integrating Cognitive Intelligence with Real-World Action

Fouad BousetouaneArXiv, Jan 2025citations 1
35

Episodic memory in AI agents poses risks that should be studied and mitigated

Chad DeChantArXiv, Jan 2025citations 1
36

A Comprehensive Survey on Long Context Language Modeling

Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Z. Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu Guo, Shilong Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie Zhu, Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Ying Tan, Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li, Zhaoxiang ZhangArXiv, Mar 2025citations 1
37

Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches

A Mumuni, F Mumuni ArXiv, Jan 2025citations 2
38

Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models

Laura Fern'andez-Becerra, Miguel Ángel González Santamarta, Á. Guerrero-Higueras, F. J. Rodríguez-Lera, Vicente Matellán OliveraArXiv, Mar 2024citations 5
39

The Evolution Beyond Search–From Information Retrieval to Autonomous Execution

A Goswami
40

Future of Information Retrieval Research in the Age of Generative AI

James Allan, Eunsol Choi, Daniel P. Lopresti, Hamed ZamaniArXiv, Dec 2024citations 4
41

Architecture of Applications Powered by Large Language Models

O Elizarov  2024
42

Exploring Conversational Agents and Continual Learning in Artificial Intelligence

B Ionescu 2024
43

Exploring data augmentation and memory strategies for AI-based synthetic personae

R Arias Gonzalez  2024
44

External Knowledge Integration in Large Language Models: A Survey on Methods, Challenges, and Future Directions

S Schindler, R Klinger
45

Tailored Query Resolution for Medical Data Interaction: Integrating LangChain4j, LLMs, and Retrieval Augmented Generation: Utilizing Real Time Embedding …

S Tegsten  2024citations 2
46

Enterprise Architecture: AI-Driven Capability Mapping for the Agile Strategic Processes

M Murtomäki  2025
47

Optimizing RAG Systems for Technical Support with LLM-based Relevance Feedback and Multi-Agent Patterns

M Finsås, J Maksim  2024citations 4
48

Explainable Artificial Intelligence: A Study of Methods, Applications, and Future Directions

G Meller
49

Large Language Models in the 6G-Enabled Computing Continuum: a White Paper

M Abel, I Ahmad, CA Casado, R Berner 2025
50

How Far Are We From AGI: Are LLMs All We Need?

T Feng, C Jin, J Liu, K Zhu, H Tu, Z Cheng 2024citations 6
)





——————
PAPERS 2:


[](query: episodic memory in AI agents OR working memory in large language models OR memory limitations in RAG systems OR external memory for RAG OR memory retention in AI OR memory-augmented neural networks agentic systems, min_year: 2024, max_year: 2025

1

Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

Y Du, W Huang, D Zheng, S MontellaArXiv, 2025
2

Cognitive memory in large language models

L ShanArXiv, 2504citations 1
3

Decentralizing AI Memory: SHIMI, a Semantic Hierarchical Memory Index for Scalable Agent Reasoning

T HelmiArXiv, 2504
4

Retrieval-Augmented Decision Transformer: External Memory for In-context RL

Thomas Schmied, Fabian Paischer, Vihang Patil, M. Hofmarcher, Razvan Pascanu, Sepp HochreiterArXiv, Oct 2024citations 9
5

Multi-LLM-Agent Systems: Techniques and Business Perspectives

Yingxuan Yang, Qiuying Peng, Jun Wang, Weinan ZhangArXiv, 2411citations 7
6

Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects

Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, Xiuqiang HeArXiv, Jan 2024citations 89
7

Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems

B Liu, J Zhang, J WangArXiv, 2025citations 5
8

Composite Learning Units: Generalized Learning Beyond Parameter Updates to Transform LLMs into Adaptive Reasoners

Santosh Kumar Radha, O. GoktasArXiv, Oct 2024citations 1
9

City Models: Past, Present and Future Prospects

Helge Ritter, O. Herzog, Kurt Rothermel, Anthony G. Cohn, Zhiqiang WuArXiv, Mar 2025
10

Position: Stop Acting Like Language Model Agents Are Normal Agents

Elija Perrier, Michael Timothy BennettArXiv, Feb 2025citations 2
11

Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey

Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, Jian-guang LouArXiv, Mar 2025citations 1
12

Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models

Y Li, X Shen, X Yao, X Ding, R KrishnanArXiv, 2025citations 1
13

LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation

Chiang Yuan, Chia-Hong Chou, Janosh RiebesellArXiv, Jan 2024citations 8
14

AI Agents for Computer Use: A Review of Instruction-based Computer Control, GUI Automation, and Operator Assistants

Pascal J. Sager, Benjamin Meyer, Peng Yan, Rebekka von Wartburg-Kottler, Layan Etaiwi, Aref Enayati, Gabriel Nobel, Ahmed Abdulkadir, Benjamin F. Grewe, Thilo StadelmannArXiv, Jan 2025citations 3
15

The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs

Z Liu, Z Gong, L Ai, Z Hui, CW LeachArXiv, 2025
16

A Survey of Conversational Search

Fengran Mo, Kelong Mao, Ziliang Zhao, Hongjin Qian, Haonan Chen, Yiruo Cheng, Xiaoxi Li, Yutao Zhu, Zhicheng Dou, Jian-Yun NieArXiv, Oct 2024citations 18
17

The application of fNIRS in studies on occupational workload: a systematic review

Robin Gemmerich, Ole Müller, Andrea SchallerFrontiers in Public Health, Apr 2025
PEER REVIEWED

18

Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications

N KrishnanArXiv, 2504
19

Memorization and Knowledge Injection in Gated LLMs

X Pan, E Hahami, Z Zhang, H SompolinskyArXiv, 2025
20

Learning through experience: Episodic memory representation for cognitive agents

S Singh, S Seshadri
21

AI Interfaces for Augmenting Episodic Memory

WD Zulfikar 2024
22

Exploring data augmentation and memory strategies for AI-based synthetic personae

R Arias Gonzalez  2024
23

… of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1 …

L Chiruzzo, A Ritter 2025
24

Advancing the Cognitive Abilities of Embodied Agents: Large-Scale Simulations and Multi-Agent Collaborations

R Gong 2024
25

AI and Language in the Urban Context: Conversational Artificial Intelligence in Cities

R Coyne 2025
)




——

[](question: What are the recent (2024-2025) developments or approaches for memory solutions in Agentic AI, RAG (Retrieval-Augmented Generation), and LLMs, especially solutions comparable to eidetic memory or addressing memory issues in these systems?

Added 25 pieces of evidence, 25 of which were relevant. Best evidence(s):

1. The excerpt discusses recent advances and considerations in AI memory systems, particularly relevant to agentic AI, Retrieval-Augmented Generation (RAG), and large language models (LLMs). Unlike stateless language and vision models, advanced AI agents incorporate memory to enable multi-step reasoning, planning, and self-improvement. Memory in these systems is categorized into short-term and long-term memory. Short-term memory supports immediate decision-making processes and includes in-context prompting strategies, which supply the model with additional examples or intermediate reasoning steps to improve performance. Long-term memory is further subdivided into experiences (past observations, actions, thoughts) and knowledge (understanding of the world and self). Experiential memories help with reflecting on task feedback and are stored as textual summaries in systems like Reflexion (2023). Generative agents document and retrieve these memories via criteria such as relevance, recency, and importance. Knowledge can be accumulated from experiences or fetched from external sources; for instance, Voyager (2023) builds a repository of executable code for skills, while methods like ReAct (2022) access APIs (e.g., Wikipedia) when external information is needed. OpenAI's ChatGPT Browse with Bing is another example enhancing response accuracy through augmented external knowledge. Retrieval-augmented methods  leverage large unstructured text corpora, and "reading to learn" approaches incorporate domain knowledge into reinforcement learning models. Memory representation occurs in two primary forms: textual memory, including unstructured natural language and structured data (e.g., tuples, databases), and parametric memory, which directly embeds knowledge into model parameters through techniques like supervised fine-tuning, knowledge editing, and model merging. Textual memory requires incorporating memory content into the context prompt during inference which adds to computational cost and latency. Parametric memory methods integrate knowledge into the model itself but can incur high training-time costs. Additionally, recent work on sparse autoencoders helps disentangle features in neural nets, enhancing interpretability which may indirectly improve memory management and reliability. The challenges addressed include enabling AI agents to simulate, store, and utilize memory more effectively to support complex reasoning and planning comparable to eidetic memory capabilities, through continuous feedback reflection, dynamic memory retrieval based on semantic criteria, and incorporation of external knowledge sources. These approaches collectively advance memory solutions in agentic AI and LLM systems towards more human-like understanding and retention.

2. Recent developments in memory solutions for Agentic AI, Retrieval-Augmented Generation (RAG), and Large Language Models (LLMs) center on maximizing the value and effective utilization of stored memory segments to enhance reasoning, decision-making, and action generation. Key challenges include preventing information overload from vast memory stores, enabling abstraction and generalization from specific memories, and addressing hallucinations or incorrect memories produced by LLMs. One prominent approach is RAG, which integrates retrieval and generation by retrieving relevant knowledge from memory stores to ground the generation process in factual information, thus improving reliability. Advanced methods incorporate self-reflection and adaptive retrieval mechanisms to decide when to generate directly or retrieve first, illustrated by models like Atlas, HippoRAG, and frameworks like RAGLAB. Long-context modeling techniques, leveraging Transformer variants such as Transformer-XL and Longformer and hierarchical or recursive methods (e.g., recurrent memory transformers), extend context windows to handle extensive memory and long-range dependencies, supporting deeper understanding in prolonged tasks. Memory compression methods like AutoCompressor and in-context autoencoders facilitate efficient long-context management. To mitigate hallucinations, fact-checking mechanisms, uncertainty estimation, and knowledge-based decoding strategies are used to ensure factual accuracy and reliability of outputs. Expert memory subnetworks such as PEER and Lamini Memory Tuning function as specialized modules that memorize specific information, thus offloading memorization from the main model to reduce hallucination. The integration of memory with other cognitive functions is crucial, where memory informs perception, planning, and action selection, and contributes to building robust internal world models that support reasoning and prediction. These world models function as structured long-term memory frameworks, influencing memory encoding and retrieval by focusing on surprising or relevant events. However, challenges remain in managing complexity, abstraction, multi-modal data, and efficient learning. Importantly, current agent memory techniques still lag behind human memory's flexibility, adaptability, associative retrieval, and sophisticated forgetting mechanisms. Future research emphasizes bio-inspired memory architectures, memory curation through metacognition, and active reflection to improve memory utilization and generalization for Agentic AI and LLMs.

3. The A-Mem model represents a recent (2025) advancement in memory systems tailored for large language model (LLM) agents. It implements an agentic memory system characterized by dynamically structuring and evolving memories as interconnected notes. Each note stores detailed interaction data with enriched attributes such as keywords, contextual descriptions, and LLM-generated tags, enabling structured and meaningful memory representation. When creating new memory entries, A-Mem leverages semantic embeddings to retrieve relevant existing notes, ensuring that relevant contextual information is linked effectively. The system employs LLMs to identify similarities and shared attributes between notes, establishing connections that evolve over time. Critically, rather than static storage, A-Mem updates existing notes dynamically upon integration of new relevant memories, refining the contextual richness and ensuring the memory structure deepens and broadens over time. Retrieval is driven by semantic similarity searches, facilitating access to pertinent historical context during agent interactions. This dynamic, evolving memory network can be viewed as a step toward achieving eidetic-like memory capabilities in AI agents by enabling richly interconnected, contextually informed, and continuously updated memory representations. This approach addresses key challenges in AI memory such as scalability, maintaining relevant context over long-term interactions, and integration of new knowledge without losing coherence. Hence, A-Mem is a significant example of recent innovations aiming to enhance memory solutions for agentic AI, RAG systems, and LLMs, making their memory-management more adaptive, contextually deep, and comparable to human-like eidetic memory capabilities.

4. The excerpt discusses the evolving landscape of memory mechanisms in AI systems driven by large language models (LLMs), highlighting shifts and innovations relevant to agentic AI, RAG, and LLM memory challenges during 2024-2025. One key development is the paradigm of shared memory, enabling multiple LLMs to access and leverage collective domain-specific expertise dynamically, improving efficiency and adaptability, and creating intelligent, resource-efficient collaborative AI networks to address complex multi-domain challenges. This collaborative approach expands AI's real-world application potential and enhances integration. Privacy concerns evolve from individual privacy to collective privacy, emphasizing the protection of group-level data usage and requiring new techniques to balance data utility with privacy, a critical consideration for memory systems becoming increasingly interconnected in agentic AI and retrieval processes. Another significant trend is the shift from rule-based AI evolution, reliant on manually crafted rules and heuristics, to automated evolution mechanisms where AI systems autonomously adapt and optimize themselves using personal and system-level memories without explicit human-defined rules. This transition leads to more dynamic, continuous self-evolving AI agents, improving responsiveness and reducing the need for human intervention. The paper also connects these advances with human memory principles—such as perceptual, working, and long-term memory—to inspire the design of more efficient, flexible AI memory architectures, proposing an eight-quadrant classification framework to conceptually organize multi-level and comprehensive memory systems. The reviewed literature from 2024-2025 includes advanced AI memory models and references current implementations such as MemoryBank (2024) and Apple's intelligence system (2024), which embed generative models with enhanced memory capabilities. Additionally, works exploring neurobiologically inspired long-term memory for LLMs , synaptic resonance approaches for contextual memory integration (2025), and key-value brain-like memory structures (2025) indicate efforts toward eidetic-like memory solutions. These emerging approaches aim to address classical AI memory issues—such as limited persistence, context retention across sessions, and scalability in memory retrieval—and augment RAG by optimizing long-term memory incorporation and multi-turn interactions, essential for agentic AI autonomous behavior and continuous learning. Collectively, these developments signal a move toward memory solutions in LLM-based agents that approximate human-like, eidetic qualities by facilitating persistent, adaptive, and contextually rich memory storage and retrieval mechanisms.

5. Recent developments in memory solutions for Agentic AI and LLMs focus on enhancing the storage, retrieval, and management of knowledge and experiential data to improve reasoning and task execution. Memory in LLM-based agents is broadly categorized into short-term and long-term memory, with additional emphasis on memory retrieval systems like Retrieval-Augmented Generation (RAG). Short-term memory stores transient, contextually relevant information within the LLM's context window, exemplified by systems like ChatDev and LangChain, which archive conversation history and distill critical interaction data to inform ongoing tasks. Long-term memory, by contrast, manages large volumes of knowledge and histories and involves interaction with external knowledge bases and databases, employing structures like knowledge graphs and vector databases. This supports agents in continuously updating and refining their understanding and capabilities, as seen in Voyager’s expanding skill repository and the ExpeL agent's experience accumulation across tasks. Reflexion stores self-reflective experiences to guide future decisions, while MemGPT manages multiple memory hierarchies to extend context beyond the LLM’s limited window, using interrupts to control flow between the system and user. A notable feature is the dynamic interaction between short-term and long-term memory, where concise, essential information from short-term memory is generalized and stored in long-term memory, maintaining the agent's evolving internal state, exemplified by Generative Agents that update their memories with new experiences aligned to natural language representations. Memory retrieval is critical, with RAG integrating retrieval modules to significantly improve output reliability by accessing relevant knowledge libraries or past experiences. Advanced memory retrieval strategies include online learning, attention mechanisms, and reinforcement learning to enable real-time model updates. For instance, LaGR-SEQ employs a reinforcement learning-based agent to decide when to query the LLM effectively, while REMEMBER equips LLMs with long-term memory capabilities to draw from previous data. Collectively, these approaches address memory constraints inherent to LLMs by providing multi-layered memory architectures that resemble eidetic qualities—capturing detailed, context-rich experiences and enabling iterative reflection and refinement of knowledge, thus mitigating memory limitations in large language models and advancing agentic AI capabilities in problem-solving and planning.

)


—————


You’re diving into a fascinating and cutting-edge area! Integrating robust memory, potentially even approaching eidetic capabilities, into agentic AI and LLMs is a significant research direction. Here’s a breakdown of current thinking and potential research avenues, keeping in mind that true “eidetic memory” as often portrayed is likely an exaggeration of human capabilities, but the _concept_ of highly accurate and detailed recall is the driving force:
Current Research Directions and Concepts:
1. Enhanced Context Windows and Retrieval:
    - A primary focus is expanding the context windows of LLMs. Larger context windows allow agents to retain more information within a single interaction.1 However, this comes with computational costs and the “lost in the middle” problem (difficulty accessing information in the middle of a long context).2
    - Research explores more efficient ways to manage and access information within these extended windows, potentially using hierarchical structures or specialized memory modules.
    - Think of this as a form of short-term, highly detailed memory within a single task or session.
2. External Memory Architectures:
    - Vector Databases (again, but with a focus on agentic use): These are crucial for long-term memory.3 Agents can store detailed observations, experiences, and learned information as vector embeddings. When faced with a new situation, they can perform semantic search to retrieve relevant memories.4
    - Knowledge Graphs: Representing information as interconnected entities and relationships allows agents to reason and retrieve complex, structured information.5 This can act as a form of highly organized long-term memory.
    - Relational Databases: For structured data, agents can learn to query and retrieve specific facts and figures, providing a precise form of memory for tabular information.
    - Dedicated Memory Modules: Some research explores creating separate neural network modules specifically designed for memory storage and retrieval, which can be integrated with the main LLM.6 These might employ different architectures optimized for memory tasks.
3. Episodic Memory Systems:
    - Inspired by human memory, episodic memory focuses on storing sequences of events with their spatial and temporal context.7
    - Research in this area for agents involves developing mechanisms to record and recall specific experiences, allowing them to learn from past trajectories and situations in a more nuanced way.8
    - This could involve storing not just the “what” but also the “when” and “where” of an event.
4. Memory Encoding and Consolidation:
    - How information is encoded into memory and how it’s strengthened or consolidated over time are important research questions.
    - This includes exploring techniques for:
        - Selective Attention: Determining what information is important enough to store in detail.
        - Memory Replay: Periodically revisiting and reinforcing stored memories.
        - Contextual Embedding: Ensuring memories are stored with relevant contextual information to facilitate accurate retrieval.
5. Memory Retrieval Mechanisms:
    - Efficient and accurate retrieval is just as crucial as storage. Research focuses on:
        - Context-Aware Retrieval: Retrieving memories based on the current situation and the agent’s goals.
        - Fuzzy Matching and Similarity Search: Retrieving relevant memories even if the current situation doesn’t perfectly match past experiences.
        - Multi-Hop Retrieval: Chaining together multiple memories to answer complex queries or make informed decisions.9
6. Neuro-Inspired Architectures:
    - Drawing inspiration from the human brain’s memory systems (hippocampus, neocortex) could lead to novel architectures for agent memory.10 This might involve exploring concepts like:
        - Sparse Distributed Representations: Encoding information in a way that is robust and allows for associative recall.11
        - Pattern Completion: The ability to retrieve a full memory from a partial cue.
          Challenges in Achieving “Eidetic-like” Memory:
- Scalability: Storing and efficiently retrieving vast amounts of detailed information is computationally expensive.
- Noise and Irrelevance: Distinguishing important details from noise and irrelevant information is crucial.
- Forgetting and Generalization: While perfect recall might seem desirable, some level of forgetting can be beneficial for generalization and extracting essential patterns.
- Integration with Reasoning: Simply having a large memory isn’t enough; the agent needs to effectively integrate retrieved information with its reasoning and decision-making processes.
  Where to Look for Research:
- arXiv: This is a primary source for pre-print research in AI, including work on agentic systems, memory architectures, and knowledge representation. Search for keywords like “agent memory,” “long-term memory for LLMs,” “episodic memory AI,” “knowledge retrieval agents,” “neuro-inspired memory AI.”
- NeurIPS, ICML, ICLR, AAAI: These top-tier AI conferences publish cutting-edge research on related topics.
- Specific Research Labs: Keep an eye on research labs known for their work in AI agents, cognitive architectures, and natural language processing.
  While achieving true eidetic memory as a perfect photographic recall might be a distant goal (and perhaps not even entirely accurate in human terms), the research into enhancing agent memory with detailed, context-aware, and efficiently retrievable information is a vibrant and crucial area for building more capable and autonomous AI systems.






————


[](query: memory systems in Agentic AI 2024|RAG (Retrieval-Augmented Generation) memory architecture 2024|long-term memory solutions for LLMs 2024|eidetic memory approaches for language models 2024|persistent memory strategies in AI agents 2024|Addressing memory limitations in generative AI 2024|scalable memory augmentation for LLMs 2024|episodic memory in AI architecture 2024|Retrieval-based memory extension for LLMs 2024|memory-centric trends in deep learning AI 2024, min_year: 2024, max_year: 2025

1

A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions

Shailja Gupta, Rajesh Ranjan, Surya Narayan SinghArXiv, Oct 2024citations 24
2

Generative to Agentic AI: Survey, Conceptualization, and Challenges

J SchneiderArXiv, 2504
3

Long Term Memory: The Foundation of AI Self-Evolution

Xun Jiang, Feng Li, Han Zhao, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize Chen, Mengyue Wu, Weizhi Ma, Mengdi Wang, Tianqiao ChenArXiv, Oct 2024citations 5
4

MemInsight: Autonomous Memory Augmentation for LLM Agents

Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine BenajibaArXiv, Mar 2025citations 1
5

On the Challenges and Opportunities in Generative AI

Laura Manduchi, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina Daubener, Sophie Fellenz, Asja Fischer, Thomas Gartner, Matthias Kirchler, M. Kloft, Yingzhen Li, Christoph Lippert, Gerard de Melo, Eric T. Nalisnick, Bjorn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich, Guy Van den Broeck, Julia E Vogt, Yixin Wang, F. Wenzel, Frank Wood, Stephan Mandt, Vincent FortuinArXiv, Feb 2024citations 32
6

Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

Y Du, W Huang, D Zheng, S MontellaArXiv, 2025
7

AI and memory

Andrew HoskinsMemory, Mind &amp; Media, Jan 2024citations 3
8

Elements of episodic memory: insights from artificial agents

Alexandria Boyle, Andrea BlomkvistPhilosophical Transactions of the Royal Society B: Biological Sciences, Sept 2024
DOMAIN LEADING

citations 4
9

A-MEM: Agentic Memory for LLM Agents

Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng ZhangArXiv, Feb 2025citations 12
10

Accelerating Retrieval-Augmented Generation

Derrick Quinn, Mohammad Nouri, Neel Patel, John Salihu, Alireza Salemi, Sukhan Lee, Hamed Zamani, Mohammad AlianProceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1, Mar 2025citations 3
11

Momentary Contexts - A Memory and Retrieval Approach for LLM Efficiency

Jaepil JeongCenter for Open Science, Dec 2024
12

LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory

Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong YuArXiv, Oct 2024citations 12
13

UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces

A Saleh, S Tarkoma, PK Donta, NH MotlaghArXiv, 2025
14

Knowledge Retrieval Based on Generative AI

Te-Lun Yang, Jyi-Shane Liu, Yuen-Hsien Tseng, Jyh-Shing Roger JangArXiv, Jan 2025citations 1
15

AI-native Memory: A Pathway from LLMs Towards AGI

Jingbo Shang, Zai Zheng, Xiang Ying, Felix Tao, Mindverse TeamArXiv, June 2024citations 7
16

Agentic AI Needs a Systems Theory

Erik Miehling, K. Ramamurthy, Kush R. Varshney, Matthew Riemer, Djallel Bouneffouf, John T. Richards, Amit Dhurandhar, Elizabeth M. Daly, Michael Hind, P. Sattigeri, Dennis Wei, Ambrish Rawat, Jasmina Gajcin, Werner GeyerArXiv, Feb 2025citations 2
17

Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented Generation

S YuArXiv, 2024citations 3
18

Retrieval-Augmented Retrieval: Large Language Models are Strong Zero-Shot Retriever

Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Yibin Lei, Tianyi Zhou, Michael Blumenstein, Daxin JiangFindings of the Association for Computational Linguistics ACL 2024, Jan 2024citations 9
19

Towards Adaptive Memory-Based Optimization for Enhanced Retrieval-Augmented Generation

Q Qin, Y Lu, Z ChuArXiv, 2504
20

"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents

Yuki Hou, Haruki Tamoto, Homei MiyashitaExtended Abstracts of the CHI Conference on Human Factors in Computing Systems, May 2024citations 30
21

The memory systems of the human brain and generative artificial intelligence

Edmund T. RollsHeliyon, June 2024
PEER REVIEWED

citations 9
22

Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives

Desta Haileselassie Hagos, Rick Battle, Danda B. RawatIEEE Transactions on Artificial Intelligence, Dec 2024
PEER REVIEWED

citations 40
23

Recall Them All: Retrieval-Augmented Language Models for Long Object List Extraction from Long Documents

Sneha Singhania, S. Razniewski, G. WeikumArXiv, May 2024citations 3
24

AI-native Memory 2.0: Second Me

Jiale Wei, Xiang Ying, Tao Gao, Fangyi Bao, Felix Tao, Jingbo ShangArXiv, Mar 2025citations 1
25

Detecting Memorization in Large Language Models

Eduardo SlonskiArXiv, Dec 2024
26

Toward Agentic AI: Generative Information Retrieval Inspired Intelligent Communications and Networking

Ruichen Zhang, Shunpu Tang, Yinqiu Liu, Dusit Niyato, Zehui Xiong, Sumei Sun, Shiwen Mao, Zhu HanArXiv, Feb 2025citations 2
27

Cognitive memory in large language models

L ShanArXiv, 2504citations 1
28

REVISITING PHOTOGRAPHIC MEMORY IN CLASH OF CHAMPIONS: HOW TO INTEGRATE PHOTOGRAPHIC MEMORY IN ENGLISH LANGUAGE TEACHING (ELT)?

Tira Nur FitriaInternational Journal of English Learning and Applied Linguistics (IJELAL), Dec 2024citations 1
29

Databases in the Era of Memory-Centric Computing

Y Chronis, A Ailamaki, L Benson, H Caminal
30

A Compressive Memory-based Retrieval Approach for Event Argument Extraction

Wanlong Liu, Enqi Zhang, Li Zhou, DingYi Zeng, Shaohuan Cheng, Chen Zhang, Malu Zhang, Wenyu ChenArXiv, Sept 2024citations 3
31

A Comparative Analysis of Episodic Memory between Humans and AI Agents with Context Correlation

S Singh, V Ghatnekar
32

Quantifying memorization and detecting training data of pre-trained language models using japanese newspaper

S Ishihara, H TakahashiArXiv, 2404citations 1
33

MARM: Unlocking the Future of Recommendation Systems through Memory Augmentation and Scalable Complexity

Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui ZhouArXiv, Nov 2024citations 4
34

Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-tuning

Bei Ouyang, Shengyuan Ye, Liekang Zeng, Tianyi Qian, Jingyi Li, Xu ChenProceedings of the 53rd International Conference on Parallel Processing, Aug 2024citations 6
35

Memory-Centric Computing: Solving Computing's Memory Problem

O Mutlu, A Olgun, IE YukselArXiv, 2505
36

B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory

L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. SoattoArXiv, July 2024citations 8
37

Learning through experience: Episodic memory representation for cognitive agents

S Singh, S Seshadri
38

The Landscape of Compute-near-memory and Compute-in-memory: A Research and Commercial Overview

Asif Ali Khan, J. P. C. D. Lima, H. Farzaneh, J. CastrillónArXiv, Jan 2024citations 17
39

Undesirable Memorization in Large Language Models: A Survey

Ali Satvaty, Suzan Verberne, Fatih TurkmenArXiv, Oct 2024citations 8
40

Toward Use of Artificial Intelligence for Advanced Persistent Threat Detection

M Walsh 2024
41

Comparative Analysis of Retrieval Augmented Generator and Traditional Large Language Models

T Oroz  2024
42

AI Interfaces for Augmenting Episodic Memory

WD Zulfikar 2024
43

Can machines (do) language? A cognitive semiotic exploration of Large Language Model-based systems, user practices and conceptions

BH Hansen  2024
44

Minimalistic Adaptive Dynamic-Programming Agents for Memory-Driven Exploration

M Pisheh Var 2024)



—————


[](query: recent developments in RAG memory 2024|compare eidetic memory AI 2024|hybrid memory architectures LLM 2024|scalable memory AI agent 2024|lifelong memory for generative models 2024|dynamic memory bank LLM 2024|context window extension solutions LLM 2024|meta-memory retrieval agentic AI 2024|synthetic long-term memory LLMs 2024|emerging trends AI memory extension 2024, min_year: 2024, max_year: 2025

1

Reimagining Memory Access for LLM Inference: Compression-Aware Memory Controller Design

Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong ZhangArXiv, Mar 2025
2

Make Your LLM Fully Utilize the Context

Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang LouArXiv, Apr 2024citations 59
3

Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory

P ChhikaraArXiv, 2025
4

LLM in a flash: Efficient Large Language Model Inference with Limited Memory

Keivan Alizadeh-Vahid, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, C. C. D. Mundo, Mohammad Rastegari, Mehrdad FarajtabarProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jan 2024citations 116
5

Vector Storage Based Long-term Memory Research on LLM

Kun Li, Xin Jing, Chengang JingInternational Journal of Advanced Network, Monitoring and Controls, Sept 2024citations 2
6

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

Yiran Ding, L. Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Mao YangArXiv, Feb 2024citations 148
7

High-speed emerging memories for AI hardware accelerators

Anni Lu, Junmo Lee, Tae-Hyeon Kim, Muhammed Ahosan Ul Karim, Rebecca Sejung Park, Harsono Simka, Shimeng YuNature Reviews Electrical Engineering, Jan 2024citations 30
8

LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning

Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-yuan Chang, Huiyuan Chen, Xia HuArXiv, Jan 2024citations 126
9

Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook

Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Lutao Jiang, Haiwei Xue, Bin Ren, D. Paudel, N. Sebe, L. V. Gool, Xuming HuArXiv, Mar 2025citations 1
10

MemoryBank: Enhancing Large Language Models with Long-Term Memory

Wanjun Zhong, Lianghong Guo, Qi-Fei Gao, He Ye, Yanlin WangProceedings of the AAAI Conference on Artificial Intelligence, Mar 2024
DOMAIN LEADING

citations 231
11

Decentralizing AI Memory: SHIMI, a Semantic Hierarchical Memory Index for Scalable Agent Reasoning

T HelmiArXiv, 2504
12

xlstm: Extended long short-term memory

M Beck, K Pöppel, M Spanring, A AuerArXiv, 2024citations 185
13

Recurrent Context Compression: Efficiently Expanding the Context Window of LLM

Chensen Huang, Guibo Zhu, Xuepeng Wang, Yifei Luo, Guojing Ge, Haoran Chen, Dong Yi, Jinqiao WangArXiv, June 2024citations 4
14

WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models

Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun ChenArXiv, May 2024citations 38
15

KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems

Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming GanArXiv, Sept 2024citations 4
16

Generative AI in Embodied Systems: System-Level Analysis of Performance, Efficiency and Scalability

Z Wan, J Qian, Y Du, Y DuArXiv, 2025
17

Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly

Peyman Hosseini, Ignacio Castro, Iacopo Ghinassi, Matthew PurverArXiv, Aug 2024citations 6
18

PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering

Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, Kam-Fai WongArXiv, Feb 2024citations 11
19

RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing

Yucheng Hu, Yuxing LuArXiv, Apr 2024citations 34
20

Towards LifeSpan Cognitive Systems

Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang, Gholamreza Haffari, Heng Ji, Julian McAuleyArXiv, Sept 2024citations 4
21

CoWPE: Adaptive Context Window Adjustment in LLMs for Complex Input Queries

Venkata Mohit TamanampudiJournal of Artificial Intelligence General science (JAIGS) ISSN:3006-4023, Aug 2024citations 3
22

From RAG to Memory: Non-Parametric Continual Learning for Large Language Models

Bernal Jim'enez Guti'errez, Yiheng Shu, Weijian Qi, Sizhe Zhou, Yu SuArXiv, Feb 2025citations 4
23

Towards Lifelong Learning of Large Language Models: A Survey

Junhao Zheng, Shengjie Qiu, Chengming Shi, Qianli MaACM Computing Surveys, Mar 2025
HIGHEST QUALITY

citations 22
24

PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs

J Malekar, P Chandarana, MH Amin, ME ElbtityArXiv, 2025
25

Imagination in Practice: The Manipulation of Objects in Mixed Reality through Eidetic Variation

F Ferro 2024
26

Scalability and Efficiency in Large-Scale Agent-Based Simulations

A Singh  2024
27

AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution

Zhiqiang Xie, Hao Kang, Ying Sheng, Tushar Krishna, Kayvon Fatahalian, Christos KozyrakisArXiv, Nov 2024citations 1
28

AI As Artificial Memory: A Global Reconfiguration of Our Collective Memory Practices?

Julien SchuhMemory Studies Review, Oct 2024citations 1
29

Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery

H Qian, Z Liu, K Mao, Z Dou 2409citations 23
30

EidetiCom: A Cross-modal Brain-Computer Semantic Communication Paradigm for Decoding Visual Perception

Linfeng Zheng, Peilin Chen, Shiqi WangArXiv, July 2024
31

Lar al memories

S Shobeiri  2024
32

Towards General Purpose Robots at Scale: Lifelong Learning and Learning to Use Memory

William YueArXiv, Dec 2024
33

AI as a Journey Toward Memory, Loss, and Nostalgia

GF Marshall  2024
34

… cloud: the value of memories a research on memory, generative AI and the potential impact of reconstructing visual representation of non documented memories

A Bocciai  2024
35

Multi-label Lifelong Machine Learning using Deep Generative Replay

MA Kassim  2024citations 1
Analysis Status
Paper Count:

79

Relevant Papers:

0

Clinical Trial Count:

0

Relevant Clinical Trials:

0

Current Evidence:

0

Disease-Target Associations:

0

03

gather evidence

question: What are the recent (2024-2025) developments or approaches for memory solutions in Agentic AI, RAG (Retrieval-Augmented Generation), and LLMs? Are there solutions comparable to eidetic memory for addressing memory issues in these systems? What are trends and evidence regarding the extension, expansion, or new approaches to memory in these models, including discussions on eidetic memory-like methods?

Added 70 pieces of evidence, 70 of which were relevant. Best evidence(s):

1. The A-Mem model represents a recent (2024-2025) development in memory solutions for agentic AI and large language models (LLMs). It introduces an agentic memory system specifically designed for LLM agents, which dynamically organizes and evolves memories as interconnected notes. Each note records agent interactions enriched with structured metadata, including keywords, contextual descriptions, and tags that are generated by the LLM itself. Upon creating new memory entries, A-Mem uses semantic embeddings to semantically retrieve relevant existing notes, enabling the system to integrate new information in relation to prior context. Moreover, A-Mem employs an LLM-driven mechanism to establish meaningful links between notes based on their similarity and shared attributes, allowing a networked memory structure to develop and evolve over time. This evolving memory architecture means that as new relevant memories are added, the system updates and refines existing notes dynamically, enhancing the contextual richness and depth of links among memories. Retrieval from memory is conducted using semantic similarity, which ensures that historically relevant context is provided during agent interactions. This approach represents a trend towards scalable, contextually rich, and dynamic memory systems that extend beyond static or single-instance recall, approximating an eidetic memory-like capability by maintaining and evolving deep, interconnected, and semantically relevant recollections. Thus, solutions like A-Mem address memory challenges in Agentic AI and RAG/LLMs by allowing continuous memory evolution, dynamic contextual enrichment, and retrieval augmentation through embedding-based semantic search, aligning with emerging trends towards more persistent, adaptive, and richly structured memory architectures in AI systems.

2. The paper by  discusses the limitations of current large language models (LLMs) regarding their effective long-context processing capability despite claims of extremely large or even unlimited context lengths. Empirical evidence from benchmarks shows current LLMs over-claim their effective context windows (e.g., GPT-4 models claimed 128K tokens but effectively handle around 64K; ChatGLM claims 128K but effectively only 4K). The authors argue that it is nearly impossible for an LLM alone to simultaneously retrieve relevant information from a very long context and perform complex reasoning in a single step for a query. Instead, they propose that Artificial General Intelligence (AGI) should be viewed as a system where the LLM acts as a core processor, short-term context serves as RAM, while a separately organized long-term memory acts like disk storage. This memory should go beyond simple raw data retrieval (as in retrieval-augmented generation or RAG) by storing important derived conclusions, reasoning outputs, and semantically connected knowledge to simplify inferences. This approach effectively integrates memory into the agent’s cognitive system, conceptually moving towards an “AI-native memory”—a lifelong, personal model that compresses various forms of memory, including types not expressible in natural language, into a deep neural network structure. The intermediate memory form might be natural language descriptions (a “Memory Palace”), facilitating user interaction and reasoning, while the ultimate goal is an individualized lifelong personal model (LPM) that can represent all memories more compactly and in an AI-native fashion. Such memory systems would enable proactive engagement, personalization, and social interaction in the AGI era but also raise privacy and security challenges. In terms of trends, the paper emphasizes that rather than relying on increasing context length alone, new memory architectures and forms of knowledge compression or extraction are key to extending the practical memory of LLM-based systems. While it does not explicitly mention eidetic memory by name, the concept of an AI-native lifelong personal model compressing all types of memory could be seen as analogous to eidetic memory-like solutions, in that they involve persistent, detailed, and accessible memory storage beyond short-term context windows. Overall, the developments show a clear trend towards integrating long-term structured memory, going beyond retrieval-based methods, and using neural compression and natural language abstractions as intermediate steps for more capable and memory-efficient AGI systems.

3. The excerpt discusses recent developments in memory mechanisms relevant to large language models (LLMs) and neural sequence models, highlighting innovations in architectures such as Recurrent Transformers, RWKV, and Mamba for improved handling of long-term dependencies and efficiency. RWKV combines recurrent neural networks (RNNs) and Transformers, featuring a time gate and state update that dynamically control information flow, allowing efficient processing of long text sequences while mitigating vanishing or exploding gradients. Mamba, based on the Selective State Space Model (SSM), integrates Transformer MLPs, SSM blocks for long-range dependency modeling, and a selective mechanism that dynamically adjusts model parameters like attention but more efficiently. These advances address challenges in capturing and processing memory within models. The discussion further distinguishes between short-term memory (based on key-value caches and hidden states limited to session cycles) and long-term memory (text-based or parameter-based), noting limitations in current approaches for durable and efficient memory storage and retrieval. Significantly, LLMs lack key human-like memory traits such as summarization, generalization, selective forgetting, and integration of new with existing knowledge, leading to continuous accumulation of information which can degrade performance. Recent trends point to memory parameterization as a promising direction for developing more human-like cognitive memory in LLMs — enabling models to summarize, integrate, and selectively forget information akin to human processes. Techniques such as LoRA (Low-Rank Adaptation), Mixture of Experts (MOE), and TTT (Task Transfer Training), as well as new module designs, have been explored to embed dynamic, adaptable memory capabilities within model parameters without undue parameter growth. While eidetic memory (highly detailed and persistent recall akin to photographic memory) is not explicitly mentioned or realized in current models, the focus on memory parameterization and dynamic information control mechanisms like those in RWKV and Mamba represent steps toward more flexible, persistent, and generalized memory systems in AI. The trajectory indicates ongoing research into expanding memory capacity and retrieval efficiency beyond session-bound caches, aiming at cognitively inspired capabilities including selective forgetting and integration to enhance large-scale model performance and applicability in agentic AI, RAG frameworks, and advanced LLMs.

4. The excerpt discusses recent developments and comparative evaluations of various memory-related techniques and models in AI, focusing on 2024 approaches toward enhancing memory capabilities in Agentic AI, Retrieval-Augmented Generation (RAG), and Large Language Models (LLMs). A key contribution includes the proposal of AI-native memory, conceptualized as a system where LLMs act as processors with their context functioning like RAM, and memory acting as disk storage. Efficient memory is highlighted as critical for progressing towards Artificial General Intelligence (AGI), with two main proposed solutions: (1) constructing a "Memory Palace" using information extraction and generation for structured storage of knowledge, and (2) compressing memory into neural network representations for efficient retrieval. These two can be combined to form a robust memory system, potentially resembling eidetic memory in terms of recall capacity and efficiency.

The evaluation reported compares several methods, including long-context LLMs (Gemini 1.5 Pro, GPT-4o), traditional RAG methods (RAG++, GraphRAG-local and global variants), and a proposed Learned Personal Memory (LPM) approach trained on augmented datasets specific to a user. Results indicate that although long-context LLMs perform reasonably well, their context length limitations lead to truncations, especially affecting time-related memory questions. RAG methods excel at answering questions tied to explicit entities or relations ('local' questions) but struggle with higher-level, global understanding ('global' questions) and recommendation tasks that require broader generalization beyond explicit context. The LPM model, despite being smaller (7B parameters) than other baselines, surpasses both long-context LLMs and RAG methods on average, showing significant advantages in memory retention, understanding, prediction, and recommendations. This suggests new neural network-based fine-tuning and data augmentation approaches offer practical, scalable memory solutions beyond traditional retrieval-augmented architectures.

Trends also show a move from purely retrieval-based memory systems to integrated memory systems that compress and embed past information to allow both local, detail-specific retrieval and global contextual understanding. This is aligned with the notion that memory in AGI should be personalized, application-agnostic, and capable of proactive interaction—enabling sustained user engagement, personalization, and privacy/security considerations. Overall, the research points toward hybrid memory architectures combining structured knowledge extraction (akin to graph-based or Memory Palace concepts) with neural compressed representations as promising avenues for overcoming current memory limitations of LLMs, moving closer to a system with eidetic memory-like qualities in Agentic AI.

5. The excerpt from Y  discusses various recent developments in memory solutions specifically relevant to agentic AI, retrieval-augmented generation (RAG), and large language models (LLMs). It highlights several approaches emerging in 2024 and 2025 to enhance memory management, updating, retrieval, and utilization within these AI systems. Key memory operations include editing, unlearning, lifelong (continual) learning, and memory-grounded generation. Notably, editing techniques focus on locating and updating specific memory entries in models, including methods like ROME, MEMIT, and AlphaEdit for system editing and KE, MEND, and DAFNET for meta learning. Unlearning addresses the intentional removal of outdated, sensitive, or harmful information, with active forgetting strategies and prompt, parameter, or regularization-based methods discussed (e.g., ICUL, ECO, ULD, EUL, TaSL). Lifelong learning techniques emphasize continual memory growth and adaptation, including replay-based and interactive learning methods such as DSI++ and LSCS. In terms of utilization, the excerpt describes memory retrieval paradigms structured into query-centered, memory-centered, and event-centered retrieval, supported by methods like FLARE, IterCQR (query-centered), Wu et al.’s indexing and reranking (memory-centered), and LoCoMo, CC, MSC (event-centered). Additionally, reasoning and integration approaches leverage structures such as multi-hop graph traversal and memory graph evolution to improve context incorporation. Regarding memory-grounded generation, three broad categories are outlined: self-reflective reasoning methods (e.g., MoT, StructRAG) that guide intermediate reasoning steps; feedback-guided correction methods (e.g., MemoRAG, Repair) that use feedback memories to improve generation robustness; and contextually-aligned long-term generation techniques (e.g., COMEDY, MemoChat, ReadAgent) that integrate compressed or summarized memory content into generation. The excerpt also discusses memory evolution paradigms: static integration, which enriches immediate contextual grounding, and dynamic memory evolution, seen in systems like A-MEM, Synapse, R2I, and SCM, which allow memory to adapt and restructure during interactions, supporting lifelong learning agents. While it does not explicitly mention eidetic memory, the described dynamic memory evolution and self-reflective reasoning approaches bear resemblance to eidetic memory by enabling systems to retain, update, verify, and integrate memories over time in a structured manner. Emerging trends focus on balancing self-organized updates with user-driven interventions, ensuring memory accuracy, privacy, and safety (including intentional memory forgetting), and expanding memory capabilities through compression, retrieval, and lifelong learning. Overall, advancements in memory solutions emphasize scalable, adaptive, and interpretable memory management for AI systems, similar in spirit to the concept of eidetic memory but realized through computational methods tailored for RAG, LLMs, and agentic AI.

)



1
Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory


P Chhikara


ArXiv, 2025
Contexts:
Used 
~1.1~
~1.2~
~1.3~
~1.4~
~1.5~
2
Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions


Y Du, W Huang, D Zheng, S Montella


ArXiv, 2025
Contexts:
Used 
~2.1~
~2.2~
~2.3~
~2.4~
~2.5~
~2.6~
~2.7~
~2.8~
Unused 
~2.9~
~2.10~
~2.11~
~2.12~
~2.13~
~2.14~
3
A-MEM: Agentic Memory for LLM Agents

Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang


ArXiv, Feb 2025

citations 12
Contexts:
Used 
~3.1~
~3.2~
~3.3~
~3.4~
Unused 
~3.5~
4
AI-native Memory: A Pathway from LLMs Towards AGI

Jingbo Shang, Zai Zheng, Xiang Ying, Felix Tao, Mindverse Team


ArXiv, June 2024

citations 7
Contexts:
Used 
~4.1~
~4.2~
~4.3~
5
AI-native Memory 2.0: Second Me

Jiale Wei, Xiang Ying, Tao Gao, Fangyi Bao, Felix Tao, Jingbo Shang


ArXiv, Mar 2025

citations 1
Contexts:
Used 
~5.1~
6
A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions

Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh


ArXiv, Oct 2024

citations 24
Contexts:
Used 
~6.1~
~6.2~
7
Generative to Agentic AI: Survey, Conceptualization, and Challenges


J Schneider


ArXiv, 2504
Contexts:
Used 
~7.1~
~7.2~
8
Cognitive memory in large language models


L Shan


ArXiv, 2504

citations 1
Contexts:
Used 
~8.1~
Unused 
~8.2~
~8.3~
~8.4~
~8.5~
~8.6~
9
Towards LifeSpan Cognitive Systems

Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang, Gholamreza Haffari, Heng Ji, Julian McAuley


ArXiv, Sept 2024

citations 4
Contexts:
Used 
~9.1~
Unused 
~9.2~
~9.3~
~9.4~







OUTPUT:



Abstract—Recent research in Agentic AI, Retrieval-Augmented Generation (RAG), and large language models (LLMs) from 2024 to 2025 has focused on overcoming intrinsic memory limitations by devising novel architectures and dynamic memory management techniques that extend, consolidate, and structure knowledge in ways that may approximate aspects of human eidetic memory. These approaches integrate extraction, consolidation, graph‐based structuring, dynamic retrieval, and even neuro‐inspired continuous learning to allow realistic long-term storage and detailed, context-rich recall. This report examines recent developments, analyzes trends in memory extension and expansion, and discusses whether solutions comparable to eidetic memory are emerging, supported by recent advancements documented in the literature.

I. Introduction  
Digital intelligent systems have seen tremendous growth in capabilities, yet the challenge of maintaining an unabridged, persistent, and dynamically updated memory remains a major bottleneck, especially when agents require long-term, context-aware interactions. Traditional techniques, including increasing context windows, have proven insufficient as they tend to degrade in performance due to attention mechanisms over extended sequences (~1.1~). The convergence of Agentic AI, RAG, and LLMs has motivated research to engineer memory systems that are not only scalable and efficient but also possess characteristics reminiscent of human eidetic memory—specifically, the ability to retain detailed, vivid, and highly accurate records for extended periods. In the following sections, we review the state-of-the-art techniques developed between 2024 and 2025, focusing on their approach to memory retention, structured recall, and dynamic updating, and assess whether any of these developments present features analogous to eidetic memory (~2.1~).

II. Memory Challenges and Limitations in Agentic AI, RAG, and LLMs  
LLMs inherently suffer from fixed context windows that restrict the coherent accumulation of dialogue history and knowledge over multiple sessions (~1.2~). Even with models claiming extended context windows, such as GPT-4 and its variants, the phenomenon of attention degradation and the loss of relevance in distant tokens remain significant obstacles (~1.1~). Moreover, RAG systems, which retrieve external knowledge to supplement prompt-based reasoning, have traditionally operated as static stores for unstructured data, often failing to integrate new insights actively or adapt previous facts based on evolving contexts (~2.2~). These deficiencies underline the need for memory solutions that go beyond mere storage and retrieval to include dynamic evolution, consolidation, and organized representation across time (guanpta2410acomprehensive survey pages 6-9).

III. Emerging Approaches for Dynamic and Scalable Memory – Toward Eidetic Features  
A. Dynamic Memory Extraction and Consolidation  
Recent works such as the Mem0 architecture have introduced paradigms where relevant pieces of conversational history are dynamically extracted and consolidated using natural language representations, leading to a dramatic reduction in token usage and latency compared to full-context models (~1.1~, ~1.3~). These systems adopt an extraction-update cycle that compares new memories against previously stored ones based on semantic similarity to decide whether to add, update, or delete items. This selective retention process is reminiscent of human cognitive processes where only salient bits of information are preserved, a behavior that partially mirrors the principles of eidetic memory (~1.2~).

B. Graph-Based Memory Structures and Memory Evolution  
More advanced variants, such as Mem0g, extend the basic extraction framework by representing conversational memories as nodes in a graph, with relationships between entities captured as edges (~1.3~). This graph-based approach allows memory systems to maintain complex relational information, thereby supporting higher-order reasoning and context linking. Systems such as A-MEM take this concept further by autonomously constructing and evolving interconnected memory networks through agentic note generation and dynamic linking (~3.1~, ~3.2~). The continuous update mechanisms resemble human memory evolution, where contextual features, keywords, and semantic relations are progressively refined and reorganized—characteristics that edge toward the fidelity and persistence seen in eidetic memory (~3.3~).

C. Hybrid Memory Architectures and Multi-Component Systems  
Another significant trend is the combination of parametric and nonparametric memory elements into hybrid architectures. Systems like those discussed in ~2.2~ integrate continual learning strategies with external memory retrieval and compression, allowing for dynamic updating and consolidation of new knowledge without full model retraining (~2.3~, ~2.1~). These hybrid systems leverage internal model parameters to capture stable, long-term patterns while augmenting this with external vector stores and knowledge graphs for precise retrieval and update operations. The interplay between the in-weights memory and the external retrieval modules exemplifies how cognitive processes can be mimicked in artificial systems, thereby suggesting pathways to approximate eidetic memory functions (~2.4~).

D. Agentic Memory in LLM Agents  
Agentic AI systems have recently been augmented with specialized memory frameworks that support proactive interactions and adaptive learning. For instance, the A-MEM architecture, inspired by the Zettelkasten method, allows agents to create rich, linked memory notes that evolve continuously over interactions. This agentic memory supports both episodic recall and higher-level abstraction, enabling AI agents to perform complex multi-hop reasoning by effectively navigating their internal knowledge graph (~3.2~, ~3.4~). Such systems often require significantly fewer tokens to maintain detailed context than traditional approaches, which is a testament to their efficiency and scalability. The dynamic linking and evolution of memory notes mirror the selective preservation and interconnection properties found in human eidetic memory (~1.4~).

E. Novel Neural and AI-native Memory Approaches  
Emerging work in AI-native memory seeks to fundamentally embed memory into neural architectures, aiming to build lifelong personal models that compress and represent user interactions in a deep network form (~4.1~, ~4.2~). Approaches such as AI-native Memory 2.0—‘SECOND ME’—leverage multi-layer memory architectures that include a raw data layer (L0), a natural language summary layer (L1), and an AI-native knowledge layer (L2) that stores information in a non-verbal, compressed neural representation (~5.1~). By doing so, these models dynamically offload and integrate historical user data, enabling proactive recall and context optimization that extend well beyond traditional retrieval methodologies. Although not explicitly labeled as ‘eidetic memory,’ the persistent, detailed, and adaptive nature of these architectures bears strong resemblance to the principles underlying eidetic recall (~4.3~).

F. Retrieval-Augmented Generation and Fused Memory Mechanisms  
RAG methodologies have evolved from simple document retrieval systems to more refined architectures that integrate closed-loop feedback with generation processes. Recent surveys have demonstrated that incorporating reflective tokens and iterative retrieval-refinement cycles dramatically improves the quality, factuality, and contextual depth of LLM outputs (~6.1~). These systems exploit external vector databases, key-value memories, and hybrid retrieval-generation pipelines to achieve scalable memory extension while controlling for noise and redundancy. Although these approaches primarily enhance retrieval accuracy, the combination of retrieval with continuous memory updating has led some researchers to propose mechanisms that could eventually approximate an eidetic-like storage capacity, wherein highly detailed and context-aware memories are maintained indefinitely in a compressed, efficient form (~7.1~).

IV. Comparisons to Eidetic Memory and Evaluative Metrics  
Eidetic memory, often defined as the near-perfect, detailed, and enduring recall found in a minority of humans, has long been a benchmark for ideal memory systems in cognitive science. While no current AI system replicates human eidetic memory completely, several emerging approaches strive to achieve analogous features. For instance, dynamic memory architectures such as Mem0 and A-MEM attempt to selectively store salient information with high granularity and rich interconnections, features that are conceptually similar to the detailed retention observed in eidetic memory (~1.1~, ~3.2~).

A critical evaluative metric in these systems is how efficiently they can retrieve contextually relevant information with minimized latency and token cost while sustaining long-term coherence. Empirical evaluations have demonstrated that architectures like A-MEM not only reduce computational overhead by more than 90% compared to full-context approaches but also deliver substantial performance improvements on complex reasoning benchmarks, suggesting that they approach a form of meticulous, persistent recollection (~1.3~, ~3.3~). Additionally, the integration of hierarchical representation, graph-based memory structuring, and continuous memory evolution are proving critical for maintaining a high level of fidelity in long-term recall—a necessary condition for any system aspiring to epitomize eidetic memory functionalities (~2.5~).

V. Trends and Future Directions  
The trends emerging from the 2024-2025 literature indicate several promising directions for extending and enhancing memory in Agentic AI, RAG, and LLMs. Among these trends are:

1. Integration of Multi-Modal Memory Components:  
   Future systems are likely to incorporate not only text-based representations but also multimodal memory, which integrates visual, audio, and structured data to form a unified contextual representation. This trend is evident in recent RAG approaches that extend beyond simple textual retrieval by embedding image and video data into the retrieval pipeline (~6.2~). Such approaches could lead to richer, more comprehensive memory systems that support a more human-like, eidetic level of detail and contextual awareness.

2. Autonomous Memory Management and Dynamic Evolution:  
   There is a growing consensus that memory systems in AI must be self-organizing and dynamically adaptive. Innovations such as the dynamic memory extraction-consolidation cycle, graph-based memory evolution, and agentic note linking suggest that memory architectures will continue to evolve autonomously. By continuously updating memory representations and restructuring internal networks in response to new data, these systems could eventually offer persistent, contextually rich memory that resembles, in part, the detailed nature of eidetic memory (~1.4~, ~3.1~).

3. Hybrid Architectures Combining Parametric and External Memory:  
   The fusion of parametric memory (embedded in model weights) with nonparametric external retrieval mechanisms appears to be a particularly promising approach for extending effective memory capacity. Hybrid models address catastrophic forgetting while supporting rapid adaptation to new information, a dual objective that is essential in any system aspiring to long-term, detailed memory recall capabilities comparable to eidetic memory (~2.6~, ~6.1~).

4. AI-Native Memory Representations and Lifelong Learning:  
   AI-native memory—where memory is represented in a compressed, non-natural language form—offers potential advantages in storage density, efficient retrieval, and long-term retention. Lifespan Cognitive Systems (LSCS) and systems like SECOND ME exemplify this trajectory by integrating aspects of continual learning, personal data compression, and adaptive memory encoding into a unified memory model that could, in future, enable near-eidetic recall capabilities (~4.1~, ~5.1~). These efforts, while still nascent, indicate a shift from static, retrieval-based memory toward systems that are intrinsically adaptable and capable of lifelong memory evolution.

5. Efficient Retrieval-Augmented Generation with Iterative Refinement:  
   Recent developments in RAG methodologies have focused on iterative retrieval and chain-of-thought prompting that efficiently integrate retrieved memories into the generation process. This hybrid approach, combining retrieval with real-time reflective reasoning, aims to minimize irrelevant noise while maximizing context fidelity. Such systems could serve as a stepping stone toward an eidetic memory engine by preserving detailed, high-quality memory fragments that are continuously updated and readily accessible for complex multi-hop reasoning tasks (~7.2~, ~6.1~).

VI. Evaluation and Performance Metrics  
Quantitative evaluations in recent studies have demonstrated that memory architectures employing dynamic updating and graph-based organizational structures achieve substantial performance gains over traditional approaches. For example, benchmarks indicate that systems like A-MEM can yield a relative improvement of up to 26% in certain evaluation metrics compared to conventional full-context methods, while simultaneously reducing latency by over 90% (~1.3~, ~3.4~). These metrics not only reflect improvements in efficiency but also suggest that the quality and granularity of memory retrieval in these systems are approaching levels necessary for more nuanced and detailed recall. Such performance characteristics are essential for memory systems that aspire to an eidetic-like quality, where every detail, once encoded, remains precisely accessible over time.

VII. Discussion  
Although true eidetic memory—characterized by unerring, detailed, and permanent recall—has yet to be achieved in AI systems, the converging trends in agentic memory architectures, dynamic memory evolution, and hybrid parametric-external memory solutions represent meaningful strides in that direction. The selective retention, autonomous reorganization, and structured recall mechanisms observed in recent architectures, such as Mem0, Mem0g, and A-MEM, provide compelling evidence that AI memory can be extended and expanded to address long-term interaction needs effectively (~1.1~, ~3.2~). Furthermore, the incorporation of features such as graph-based memory networks, iterative retrieval-refinement cycles, and AI-native neural compression fosters an environment where detailed and persistent memory storage is gradually emerging as a viable possibility.

These advancements point to a future where memory solutions in Agentic AI and RAG systems will be capable of dynamically encoding complex contextual information, continuously integrating new experiences, and retrieving detailed content with high fidelity. In this sense, while the current systems do not yet reproduce true eidetic memory, they offer architectures and operational paradigms that are arguably the most promising steps toward attaining such capabilities (~2.7~, ~8.1~).

VIII. Conclusion  
In conclusion, recent developments in memory solutions for Agentic AI, RAG, and LLMs during 2024–2025 clearly indicate that the field is moving toward architectures that extend and expand memory capabilities in ways that could, in part, be considered analogous to eidetic memory. Approaches such as dynamic memory extraction, graph-based memory consolidation, hybrid parametric-external architectures, and AI-native memory representations collectively advance the state-of-the-art. Although a fully eidetic memory system remains an aspirational goal, the trends and empirical evidence suggest that continuous improvements in structured memory management and adaptive retrieval methods may eventually deliver systems with persistent, detailed, and contextually rich memory—hallmarks of the long-sought eidetic memory in human cognition (~1.5~, ~2.8~, ~4.2~).

Future research is expected to further explore the integration of multimodal data, lifelong learning, and efficient memory updating techniques to overcome the remaining challenges, including catastrophic forgetting, scalability of memory representations, and efficient long-term context management. In this evolving landscape, advances in agent-centric memory architectures and retrieval-augmented frameworks will play a pivotal role in shaping AI systems capable of sustained, coherent, and human-like long-term interaction—a vision that continues to push the boundaries of what is achievable in artificial memory systems (~9.1~, ~5.1~, ~3.1~).

Overall, while no current system has yet achieved a complete eidetic memory analogous to that observed in some humans, the convergence of innovative memory management techniques discussed in the recent literature indicates that we are on the cusp of realizing AI memory mechanisms that are progressively more detailed, persistent, and adaptive. These developments represent an encouraging step toward resolving longstanding memory bottlenecks in LLMs and Agentic AI, ultimately supporting more coherent, contextually grounded, and intelligent agent behaviors.

Keywords—Agentic AI, Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Eidetic Memory, Dynamic Memory Consolidation, Graph-Based Memory, Hybrid Architectures.









Abstract—Recent research in Agentic AI, Retrieval-Augmented Generation (RAG), and large language models (LLMs) from 2024 to 2025 has focused on overcoming intrinsic memory limitations by devising novel architectures and dynamic memory management techniques that extend, consolidate, and structure knowledge in ways that may approximate aspects of human eidetic memory. These approaches integrate extraction, consolidation, graph‐based structuring, dynamic retrieval, and even neuro‐inspired continuous learning to allow realistic long-term storage and detailed, context-rich recall. This report examines recent developments, analyzes trends in memory extension and expansion, and discusses whether solutions comparable to eidetic memory are emerging, supported by recent advancements documented in the literature.
I. Introduction
Digital intelligent systems have seen tremendous growth in capabilities, yet the challenge of maintaining an unabridged, persistent, and dynamically updated memory remains a major bottleneck, especially when agents require long-term, context-aware interactions. Traditional techniques, including increasing context windows, have proven insufficient as they tend to degrade in performance due to attention mechanisms over extended sequences (1.1). The convergence of Agentic AI, RAG, and LLMs has motivated research to engineer memory systems that are not only scalable and efficient but also possess characteristics reminiscent of human eidetic memory—specifically, the ability to retain detailed, vivid, and highly accurate records for extended periods. In the following sections, we review the state-of-the-art techniques developed between 2024 and 2025, focusing on their approach to memory retention, structured recall, and dynamic updating, and assess whether any of these developments present features analogous to eidetic memory (2.1).
II. Memory Challenges and Limitations in Agentic AI, RAG, and LLMs
LLMs inherently suffer from fixed context windows that restrict the coherent accumulation of dialogue history and knowledge over multiple sessions (1.2). Even with models claiming extended context windows, such as GPT-4 and its variants, the phenomenon of attention degradation and the loss of relevance in distant tokens remain significant obstacles (1.1). Moreover, RAG systems, which retrieve external knowledge to supplement prompt-based reasoning, have traditionally operated as static stores for unstructured data, often failing to integrate new insights actively or adapt previous facts based on evolving contexts (2.2). These deficiencies underline the need for memory solutions that go beyond mere storage and retrieval to include dynamic evolution, consolidation, and organized representation across time (guanpta2410acomprehensive survey pages 6-9).
III. Emerging Approaches for Dynamic and Scalable Memory – Toward Eidetic Features
A. Dynamic Memory Extraction and Consolidation
Recent works such as the Mem0 architecture have introduced paradigms where relevant pieces of conversational history are dynamically extracted and consolidated using natural language representations, leading to a dramatic reduction in token usage and latency compared to full-context models (1.1, 1.3). These systems adopt an extraction-update cycle that compares new memories against previously stored ones based on semantic similarity to decide whether to add, update, or delete items. This selective retention process is reminiscent of human cognitive processes where only salient bits of information are preserved, a behavior that partially mirrors the principles of eidetic memory (1.2).
B. Graph-Based Memory Structures and Memory Evolution
More advanced variants, such as Mem0g, extend the basic extraction framework by representing conversational memories as nodes in a graph, with relationships between entities captured as edges (1.3). This graph-based approach allows memory systems to maintain complex relational information, thereby supporting higher-order reasoning and context linking. Systems such as A-MEM take this concept further by autonomously constructing and evolving interconnected memory networks through agentic note generation and dynamic linking (3.1, 3.2). The continuous update mechanisms resemble human memory evolution, where contextual features, keywords, and semantic relations are progressively refined and reorganized—characteristics that edge toward the fidelity and persistence seen in eidetic memory (3.3).
C. Hybrid Memory Architectures and Multi-Component Systems
Another significant trend is the combination of parametric and nonparametric memory elements into hybrid architectures. Systems like those discussed in 2.2 integrate continual learning strategies with external memory retrieval and compression, allowing for dynamic updating and consolidation of new knowledge without full model retraining (2.3, 2.1). These hybrid systems leverage internal model parameters to capture stable, long-term patterns while augmenting this with external vector stores and knowledge graphs for precise retrieval and update operations. The interplay between the in-weights memory and the external retrieval modules exemplifies how cognitive processes can be mimicked in artificial systems, thereby suggesting pathways to approximate eidetic memory functions (2.4).
D. Agentic Memory in LLM Agents
Agentic AI systems have recently been augmented with specialized memory frameworks that support proactive interactions and adaptive learning. For instance, the A-MEM architecture, inspired by the Zettelkasten method, allows agents to create rich, linked memory notes that evolve continuously over interactions. This agentic memory supports both episodic recall and higher-level abstraction, enabling AI agents to perform complex multi-hop reasoning by effectively navigating their internal knowledge graph (3.2, 3.4). Such systems often require significantly fewer tokens to maintain detailed context than traditional approaches, which is a testament to their efficiency and scalability. The dynamic linking and evolution of memory notes mirror the selective preservation and interconnection properties found in human eidetic memory (1.4).
E. Novel Neural and AI-native Memory Approaches
Emerging work in AI-native memory seeks to fundamentally embed memory into neural architectures, aiming to build lifelong personal models that compress and represent user interactions in a deep network form (4.1, 4.2). Approaches such as AI-native Memory 2.0—‘SECOND ME’—leverage multi-layer memory architectures that include a raw data layer (L0), a natural language summary layer (L1), and an AI-native knowledge layer (L2) that stores information in a non-verbal, compressed neural representation (5.1). By doing so, these models dynamically offload and integrate historical user data, enabling proactive recall and context optimization that extend well beyond traditional retrieval methodologies. Although not explicitly labeled as ‘eidetic memory,’ the persistent, detailed, and adaptive nature of these architectures bears strong resemblance to the principles underlying eidetic recall (4.3).
F. Retrieval-Augmented Generation and Fused Memory Mechanisms
RAG methodologies have evolved from simple document retrieval systems to more refined architectures that integrate closed-loop feedback with generation processes. Recent surveys have demonstrated that incorporating reflective tokens and iterative retrieval-refinement cycles dramatically improves the quality, factuality, and contextual depth of LLM outputs (6.1). These systems exploit external vector databases, key-value memories, and hybrid retrieval-generation pipelines to achieve scalable memory extension while controlling for noise and redundancy. Although these approaches primarily enhance retrieval accuracy, the combination of retrieval with continuous memory updating has led some researchers to propose mechanisms that could eventually approximate an eidetic-like storage capacity, wherein highly detailed and context-aware memories are maintained indefinitely in a compressed, efficient form (7.1).
IV. Comparisons to Eidetic Memory and Evaluative Metrics
Eidetic memory, often defined as the near-perfect, detailed, and enduring recall found in a minority of humans, has long been a benchmark for ideal memory systems in cognitive science. While no current AI system replicates human eidetic memory completely, several emerging approaches strive to achieve analogous features. For instance, dynamic memory architectures such as Mem0 and A-MEM attempt to selectively store salient information with high granularity and rich interconnections, features that are conceptually similar to the detailed retention observed in eidetic memory (1.1, 3.2).
A critical evaluative metric in these systems is how efficiently they can retrieve contextually relevant information with minimized latency and token cost while sustaining long-term coherence. Empirical evaluations have demonstrated that architectures like A-MEM not only reduce computational overhead by more than 90% compared to full-context approaches but also deliver substantial performance improvements on complex reasoning benchmarks, suggesting that they approach a form of meticulous, persistent recollection (1.3, 3.3). Additionally, the integration of hierarchical representation, graph-based memory structuring, and continuous memory evolution are proving critical for maintaining a high level of fidelity in long-term recall—a necessary condition for any system aspiring to epitomize eidetic memory functionalities (2.5).
V. Trends and Future Directions
The trends emerging from the 2024-2025 literature indicate several promising directions for extending and enhancing memory in Agentic AI, RAG, and LLMs. Among these trends are:
1. Integration of Multi-Modal Memory Components:
   Future systems are likely to incorporate not only text-based representations but also multimodal memory, which integrates visual, audio, and structured data to form a unified contextual representation. This trend is evident in recent RAG approaches that extend beyond simple textual retrieval by embedding image and video data into the retrieval pipeline (6.2). Such approaches could lead to richer, more comprehensive memory systems that support a more human-like, eidetic level of detail and contextual awareness.
2. Autonomous Memory Management and Dynamic Evolution:
   There is a growing consensus that memory systems in AI must be self-organizing and dynamically adaptive. Innovations such as the dynamic memory extraction-consolidation cycle, graph-based memory evolution, and agentic note linking suggest that memory architectures will continue to evolve autonomously. By continuously updating memory representations and restructuring internal networks in response to new data, these systems could eventually offer persistent, contextually rich memory that resembles, in part, the detailed nature of eidetic memory (1.4, 3.1).
3. Hybrid Architectures Combining Parametric and External Memory:
   The fusion of parametric memory (embedded in model weights) with nonparametric external retrieval mechanisms appears to be a particularly promising approach for extending effective memory capacity. Hybrid models address catastrophic forgetting while supporting rapid adaptation to new information, a dual objective that is essential in any system aspiring to long-term, detailed memory recall capabilities comparable to eidetic memory (2.6, 6.1).
4. AI-Native Memory Representations and Lifelong Learning:
   AI-native memory—where memory is represented in a compressed, non-natural language form—offers potential advantages in storage density, efficient retrieval, and long-term retention. Lifespan Cognitive Systems (LSCS) and systems like SECOND ME exemplify this trajectory by integrating aspects of continual learning, personal data compression, and adaptive memory encoding into a unified memory model that could, in future, enable near-eidetic recall capabilities (4.1, 5.1). These efforts, while still nascent, indicate a shift from static, retrieval-based memory toward systems that are intrinsically adaptable and capable of lifelong memory evolution.
5. Efficient Retrieval-Augmented Generation with Iterative Refinement:
   Recent developments in RAG methodologies have focused on iterative retrieval and chain-of-thought prompting that efficiently integrate retrieved memories into the generation process. This hybrid approach, combining retrieval with real-time reflective reasoning, aims to minimize irrelevant noise while maximizing context fidelity. Such systems could serve as a stepping stone toward an eidetic memory engine by preserving detailed, high-quality memory fragments that are continuously updated and readily accessible for complex multi-hop reasoning tasks (7.2, 6.1).
   VI. Evaluation and Performance Metrics
   Quantitative evaluations in recent studies have demonstrated that memory architectures employing dynamic updating and graph-based organizational structures achieve substantial performance gains over traditional approaches. For example, benchmarks indicate that systems like A-MEM can yield a relative improvement of up to 26% in certain evaluation metrics compared to conventional full-context methods, while simultaneously reducing latency by over 90% (1.3, 3.4). These metrics not only reflect improvements in efficiency but also suggest that the quality and granularity of memory retrieval in these systems are approaching levels necessary for more nuanced and detailed recall. Such performance characteristics are essential for memory systems that aspire to an eidetic-like quality, where every detail, once encoded, remains precisely accessible over time.
   VII. Discussion
   Although true eidetic memory—characterized by unerring, detailed, and permanent recall—has yet to be achieved in AI systems, the converging trends in agentic memory architectures, dynamic memory evolution, and hybrid parametric-external memory solutions represent meaningful strides in that direction. The selective retention, autonomous reorganization, and structured recall mechanisms observed in recent architectures, such as Mem0, Mem0g, and A-MEM, provide compelling evidence that AI memory can be extended and expanded to address long-term interaction needs effectively (1.1, 3.2). Furthermore, the incorporation of features such as graph-based memory networks, iterative retrieval-refinement cycles, and AI-native neural compression fosters an environment where detailed and persistent memory storage is gradually emerging as a viable possibility.
   These advancements point to a future where memory solutions in Agentic AI and RAG systems will be capable of dynamically encoding complex contextual information, continuously integrating new experiences, and retrieving detailed content with high fidelity. In this sense, while the current systems do not yet reproduce true eidetic memory, they offer architectures and operational paradigms that are arguably the most promising steps toward attaining such capabilities (2.7, 8.1).
   VIII. Conclusion
   In conclusion, recent developments in memory solutions for Agentic AI, RAG, and LLMs during 2024–2025 clearly indicate that the field is moving toward architectures that extend and expand memory capabilities in ways that could, in part, be considered analogous to eidetic memory. Approaches such as dynamic memory extraction, graph-based memory consolidation, hybrid parametric-external architectures, and AI-native memory representations collectively advance the state-of-the-art. Although a fully eidetic memory system remains an aspirational goal, the trends and empirical evidence suggest that continuous improvements in structured memory management and adaptive retrieval methods may eventually deliver systems with persistent, detailed, and contextually rich memory—hallmarks of the long-sought eidetic memory in human cognition (1.5, 2.8, 4.2).
   Future research is expected to further explore the integration of multimodal data, lifelong learning, and efficient memory updating techniques to overcome the remaining challenges, including catastrophic forgetting, scalability of memory representations, and efficient long-term context management. In this evolving landscape, advances in agent-centric memory architectures and retrieval-augmented frameworks will play a pivotal role in shaping AI systems capable of sustained, coherent, and human-like long-term interaction—a vision that continues to push the boundaries of what is achievable in artificial memory systems (9.1, 5.1, 3.1).
   Overall, while no current system has yet achieved a complete eidetic memory analogous to that observed in some humans, the convergence of innovative memory management techniques discussed in the recent literature indicates that we are on the cusp of realizing AI memory mechanisms that are progressively more detailed, persistent, and adaptive. These developments represent an encouraging step toward resolving longstanding memory bottlenecks in LLMs and Agentic AI, ultimately supporting more coherent, contextually grounded, and intelligent agent behaviors.
   Keywords—Agentic AI, Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Eidetic Memory, Dynamic Memory Consolidation, Graph-Based Memory, Hybrid Architectures.





-----



Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory

P ChhikaraArXiv, 2025
Contexts:
Used 1.1
1.2
1.3
1.4
1.5
2

Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

Y Du, W Huang, D Zheng, S MontellaArXiv, 2025
Contexts:
Used 2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
Unused 2.9
2.10
2.11
2.12
2.13
2.14
3

A-MEM: Agentic Memory for LLM Agents

Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng ZhangArXiv, Feb 2025citations 12
Contexts:
Used 3.1
3.2
3.3
3.4
Unused 3.5
4

AI-native Memory: A Pathway from LLMs Towards AGI

Jingbo Shang, Zai Zheng, Xiang Ying, Felix Tao, Mindverse TeamArXiv, June 2024citations 7
Contexts:
Used 4.1
4.2
4.3
5

AI-native Memory 2.0: Second Me

Jiale Wei, Xiang Ying, Tao Gao, Fangyi Bao, Felix Tao, Jingbo ShangArXiv, Mar 2025citations 1
Contexts:
Used 5.1
6

A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions

Shailja Gupta, Rajesh Ranjan, Surya Narayan SinghArXiv, Oct 2024citations 24
Contexts:
Used 6.1
6.2
7

Generative to Agentic AI: Survey, Conceptualization, and Challenges

J SchneiderArXiv, 2504
Contexts:
Used 7.1
7.2
8

Cognitive memory in large language models

L ShanArXiv, 2504citations 1
Contexts:
Used 8.1
Unused 8.2
8.3
8.4
8.5
8.6
9

Towards LifeSpan Cognitive Systems

Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang, Gholamreza Haffari, Heng Ji, Julian McAuleyArXiv, Sept 2024citations 4
Contexts:
Used 9.1
